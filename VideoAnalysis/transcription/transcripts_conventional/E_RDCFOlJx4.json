{"video_id": "E_RDCFOlJx4", "url": "https://www.youtube.com/watch?v=E_RDCFOlJx4", "title": "E_RDCFOlJx4", "category": "Unknown Category", "fps": null, "segments": [{"start": 0.88, "end": 5.0, "text": "Let's talk about the Linear Quadratic Regulator, or LQR control."}, {"start": 5.0, "end": 10.0, "text": "LQR is a type of optimal control that is based on state space representation."}, {"start": 10.0, "end": 13.66, "text": "In this video I want to introduce this topic at a very high level so that you walk away"}, {"start": 13.66, "end": 17.82, "text": "with a general understanding of the control problem and can build on this understanding"}, {"start": 17.82, "end": 20.080000000000002, "text": "when you're studying the math behind it."}, {"start": 20.080000000000002, "end": 25.04, "text": "I'll cover what it means to be optimal, how to think about the LQR problem, and then I'll"}, {"start": 25.04, "end": 29.0, "text": "show you some examples in MATLAB that I think will help you gain a little intuition about"}, {"start": 29.0, "end": 30.0, "text": "LQR."}, {"start": 30.0, "end": 34.06, "text": "I'm Brian, and welcome to a MATLAB Tech Talk."}, {"start": 34.06, "end": 37.82, "text": "To begin, let's compare the structure of the pole placement controller that we covered"}, {"start": 37.82, "end": 40.94, "text": "in the second video and an LQR controller."}, {"start": 40.94, "end": 44.46, "text": "That way you have some kind of an idea of how they're different."}, {"start": 44.46, "end": 48.58, "text": "With pole placement we found that if we feed back every state in the state vector and multiply"}, {"start": 48.58, "end": 53.400000000000006, "text": "them by a gain matrix K, we have the ability to place the closed loop poles anywhere we"}, {"start": 53.400000000000006, "end": 57.8, "text": "choose, assuming the system is controllable and observable."}, {"start": 57.8, "end": 61.699999999999996, "text": "Then we scaled the reference term to ensure that we have no steady state reference tracking"}, {"start": 61.699999999999996, "end": 62.86, "text": "error."}, {"start": 62.86, "end": 68.53999999999999, "text": "Now, the LQR structure on the other hand feeds back the full state vector, then multiplies"}, {"start": 68.53999999999999, "end": 72.52, "text": "it by a gain matrix K and subtracts it from the scaled reference."}, {"start": 72.52, "end": 78.53999999999999, "text": "So as you can see, the structure of these two control laws are completely diff- well, actually"}, {"start": 78.53999999999999, "end": 80.22, "text": "no, they're exactly the same."}, {"start": 80.22, "end": 85.6, "text": "They are both full state feedback controllers, and we can implement the results with the same"}, {"start": 85.6, "end": 89.5, "text": "structure from both LQR and pole placement."}, {"start": 89.5, "end": 91.46, "text": "Now, a quick side note about this structure."}, {"start": 91.46, "end": 95.89999999999999, "text": "We could have set it up to feed back the integral of the output, or we could have applied the"}, {"start": 95.89999999999999, "end": 98.38, "text": "gain to the state error."}, {"start": 98.38, "end": 103.11999999999999, "text": "All three of these implementations can produce zero steady state error and can be used with"}, {"start": 103.11999999999999, "end": 105.78, "text": "the results from pole placement or LQR."}, {"start": 105.78, "end": 109.41999999999999, "text": "And if you want to learn more about these other two feedback structures, I left a good source"}, {"start": 109.42, "end": 110.42, "text": "in the description."}, {"start": 110.42, "end": 113.06, "text": "Okay, we're back."}, {"start": 113.06, "end": 116.8, "text": "So why are we giving these two controllers different names if they're implemented in"}, {"start": 116.8, "end": 118.56, "text": "the exact same way?"}, {"start": 118.56, "end": 119.92, "text": "Well here's the key."}, {"start": 119.92, "end": 125.44, "text": "The implementation is the same, but how we choose K is different."}, {"start": 125.44, "end": 130.24, "text": "With pole placement, we solved for K by choosing where we want to put the closed loop poles."}, {"start": 130.24, "end": 134.36, "text": "We wanted to place them in a specific spot, and this was awesome."}, {"start": 134.36, "end": 138.72, "text": "But one problem with this method is figuring out where a good place is for those closed loop"}, {"start": 138.72, "end": 139.72, "text": "poles."}, {"start": 139.72, "end": 144.4, "text": "And this might not be terribly intuitive for high order systems and systems with multiple"}, {"start": 144.4, "end": 145.4, "text": "actuators."}, {"start": 145.4, "end": 149.04, "text": "So, with LQR we don't pick pole locations."}, {"start": 149.04, "end": 154.72, "text": "We find the optimal K matrix by choosing closed loop characteristics that are important to us."}, {"start": 154.72, "end": 159.34, "text": "Specifically, how well the system performs and how much effort does it take to get that"}, {"start": 159.34, "end": 160.78, "text": "performance."}, {"start": 160.78, "end": 165.07999999999998, "text": "And this statement might not make a lot of sense, so let's walk through a quick thought exercise"}, {"start": 165.07999999999998, "end": 166.68, "text": "that I think will help."}, {"start": 166.68, "end": 170.68, "text": "From borrowing and modifying this example from Christopher Lum, who has his own video on"}, {"start": 170.68, "end": 174.92000000000002, "text": "LQR that is worth watching if you want a more in-depth explanation of the mathematics."}, {"start": 174.92000000000002, "end": 177.28, "text": "I've linked to his video in the description."}, {"start": 177.28, "end": 179.68, "text": "But here's the general idea."}, {"start": 179.68, "end": 183.72, "text": "Let's say you're trying to figure out the best way, or the most optimal way, to get from"}, {"start": 183.72, "end": 184.68, "text": "your home to work."}, {"start": 184.68, "end": 188.04000000000002, "text": "And you have several transportation options to choose from."}, {"start": 188.04000000000002, "end": 192.68, "text": "You could drive your car, you could ride your bike, take the bus, or charter a helicopter."}, {"start": 192.68, "end": 196.68, "text": "And the question is, which is the most optimal choice?"}, {"start": 196.68, "end": 200.92000000000002, "text": "Well, that question by itself can't be answered, because I haven't told you what a good outcome"}, {"start": 200.92000000000002, "end": 201.92000000000002, "text": "means."}, {"start": 201.92000000000002, "end": 206.28, "text": "All of those options can get us from home to work, but they do so differently, and we need"}, {"start": 206.28, "end": 208.68, "text": "to figure out what's important to us."}, {"start": 208.68, "end": 214.04000000000002, "text": "If I said that time is the most important thing, get to work as fast as possible, then the optimal"}, {"start": 214.04000000000002, "end": 216.68, "text": "solution would be to take the helicopter."}, {"start": 216.68, "end": 220.20000000000002, "text": "On the other hand, if I said that you don't have much money and getting to work as cheaply"}, {"start": 220.20000000000002, "end": 224.68, "text": "as possible was a good outcome, then riding your bike would be the optimal solution."}, {"start": 224.68, "end": 228.92000000000002, "text": "Of course, in real life you don't have infinite money to maximize performance, and you don't"}, {"start": 228.92000000000002, "end": 231.6, "text": "have unlimited time to minimize spending."}, {"start": 231.6, "end": 234.5, "text": "But rather you're trying to find a balance between the two."}, {"start": 234.5, "end": 238.48000000000002, "text": "So maybe you'd reason that you have an early meeting, and therefore value the time it takes"}, {"start": 238.48000000000002, "end": 239.68, "text": "to get to work."}, {"start": 239.68, "end": 243.94, "text": "But you're not independently wealthy, so you care about how much money it takes."}, {"start": 243.94, "end": 249.0, "text": "Therefore the optimal solution would be to take your car, or to take the bus."}, {"start": 249.0, "end": 254.06, "text": "Now if we wanted a fancy way to mathematically assess which mode of transportation is optimal,"}, {"start": 254.06, "end": 258.68, "text": "we could set up a function that adds together the travel time and the amount of money that"}, {"start": 258.68, "end": 260.62, "text": "each option takes."}, {"start": 260.62, "end": 265.48, "text": "And then we can set the importance of time versus money with a multiplier."}, {"start": 265.48, "end": 269.26, "text": "We'll weight each of these matrices based on our own personal preferences."}, {"start": 269.26, "end": 273.62, "text": "We'll call this the cost function, or the objective function."}, {"start": 273.62, "end": 277.16, "text": "And you can see that it's heavily influenced by these weighting parameters."}, {"start": 277.16, "end": 281.42, "text": "If Q is high, then we're penalizing options that take more time, and if R is high, then"}, {"start": 281.42, "end": 284.66, "text": "we're penalizing options that cost a lot of money."}, {"start": 284.66, "end": 290.18, "text": "And once we set the weights, we calculate the total cost for each option and choose the"}, {"start": 290.18, "end": 292.90000000000003, "text": "one that has the lowest overall cost."}, {"start": 292.90000000000003, "end": 295.82, "text": "And this is the optimal solution."}, {"start": 295.82, "end": 299.5, "text": "And what's interesting about this is that there's different optimal solutions based on"}, {"start": 299.5, "end": 302.76, "text": "the relative weights you attach to performance and spending."}, {"start": 302.76, "end": 308.42, "text": "There's no universal optimal solution, just the best one given the desires of the user."}, {"start": 308.42, "end": 313.40000000000003, "text": "A CEO might take a helicopter, whereas a college student might ride a bicycle."}, {"start": 313.40000000000003, "end": 316.82, "text": "But both are optimal given their preferences."}, {"start": 316.82, "end": 321.48, "text": "And this is exactly the same kind of reasoning we do when designing a control system."}, {"start": 321.48, "end": 325.86, "text": "Rather than think about pole locations, we can think about and assess what is important"}, {"start": 325.86, "end": 332.3, "text": "to us between how well the system performs and how much we want to spend to get that performance."}, {"start": 332.3, "end": 337.3, "text": "Of course, usually how much we want to spend is not measured in dollars, but in actuator effort,"}, {"start": 337.3, "end": 339.86, "text": "or the amount of energy it takes."}, {"start": 339.86, "end": 344.06, "text": "And this is how LQR approaches finding the optimal gain matrix."}, {"start": 344.06, "end": 348.06, "text": "We set up a cost function that adds up the weighted sum of performance and effort over"}, {"start": 348.06, "end": 349.18, "text": "all time."}, {"start": 349.18, "end": 355.0, "text": "And then by solving the LQR problem, it returns the gain matrix that produces the lowest cost,"}, {"start": 355.0, "end": 357.3, "text": "given the dynamics of the system."}, {"start": 357.3, "end": 361.42, "text": "Now, the cost function that we use with LQR looks a little different than the function"}, {"start": 361.42, "end": 363.54, "text": "we developed for the travel example."}, {"start": 363.54, "end": 365.68, "text": "But the concept is exactly the same."}, {"start": 365.68, "end": 370.5, "text": "We penalize bad performance by adjusting Q, and we penalize actuator effort by adjusting"}, {"start": 370.5, "end": 371.88, "text": "R."}, {"start": 371.88, "end": 376.44, "text": "So let's look at what performance means for this cost function."}, {"start": 376.44, "end": 378.76, "text": "Performance is judged on the state vector."}, {"start": 378.76, "end": 383.08000000000004, "text": "For now, let's assume that we want every state to be zero, to be driven back to its starting"}, {"start": 383.08000000000004, "end": 384.72, "text": "equilibrium point."}, {"start": 384.72, "end": 390.12, "text": "So if the system is initialized in some non-zero state, the faster it returns to zero, the better"}, {"start": 390.12, "end": 393.1, "text": "the performance is, and the lower the cost."}, {"start": 393.1, "end": 397.1, "text": "And the way we can get a measure of how quickly it's returning to the desired state is by looking"}, {"start": 397.1, "end": 399.32000000000005, "text": "at the area under the curve."}, {"start": 399.32000000000005, "end": 401.32000000000005, "text": "This is what the integral is doing."}, {"start": 401.32000000000005, "end": 406.04, "text": "A curve with less area means that it spends more time closer to the goal than a curve with"}, {"start": 406.04, "end": 407.32000000000005, "text": "more area."}, {"start": 407.32000000000005, "end": 412.52000000000004, "text": "However, states can be negative or positive, and we don't want negative values subtracting from"}, {"start": 412.52, "end": 413.96, "text": "the overall cost."}, {"start": 413.96, "end": 417.24, "text": "So we square the value to ensure that it's positive."}, {"start": 417.24, "end": 422.18, "text": "This has the effect of punishing larger errors disproportionately more than smaller ones."}, {"start": 422.18, "end": 428.65999999999997, "text": "But it's a good compromise because it turns our cost function into a quadratic function."}, {"start": 428.65999999999997, "end": 433.71999999999997, "text": "Quadratic functions like z equals x squared plus y squared are convexed, and therefore have"}, {"start": 433.71999999999997, "end": 435.7, "text": "a definite minimum value."}, {"start": 435.7, "end": 440.97999999999996, "text": "quadratic functions that are subject to linear dynamics remain quadratic, so our system will"}, {"start": 440.97999999999996, "end": 443.42, "text": "also have a definite minimum value."}, {"start": 443.42, "end": 448.58, "text": "Lastly, we want to have the ability to weight the relative importance of each state."}, {"start": 448.58, "end": 452.88, "text": "And therefore, q isn't a single number, but a square matrix that has the same number of"}, {"start": 452.88, "end": 455.36, "text": "rows as states."}, {"start": 455.36, "end": 459.96000000000004, "text": "The q matrix needs to be positive definite so that when we multiply it with the state vectors,"}, {"start": 459.96000000000004, "end": 463.62, "text": "the resulting value is positive and non-zero."}, {"start": 463.62, "end": 468.2, "text": "And often it's just a diagonal matrix with positive values along the diagonal."}, {"start": 468.2, "end": 474.68, "text": "With this matrix, we can target the states where we want really low error by making the corresponding value in the q"}, {"start": 474.68, "end": 476.46000000000004, "text": "matrix really large."}, {"start": 476.46000000000004, "end": 480.88, "text": "And the states that we don't care about as much make those values really small."}, {"start": 480.88, "end": 484.26, "text": "The other half of the cost function adds up the cost of actuation."}, {"start": 484.26, "end": 488.62, "text": "In a very similar fashion, we look at the input vector, and we square the terms to ensure that"}, {"start": 488.62, "end": 493.46, "text": "they're positive, and then weight them with an r matrix that has positive multipliers along"}, {"start": 493.46, "end": 495.4, "text": "its diagonal."}, {"start": 495.4, "end": 498.86, "text": "We can write this in larger matrix form as follows."}, {"start": 498.86, "end": 503.32, "text": "And while you don't see the cost function written like this often, it helps us visualize"}, {"start": 503.32, "end": 504.32, "text": "something."}, {"start": 504.32, "end": 508.42, "text": "q and r are part of this larger weighting matrix."}, {"start": 508.42, "end": 511.68, "text": "But the off-diagonal terms of this matrix are zero."}, {"start": 511.68, "end": 517.78, "text": "And we can fill in those corners with n, such that the overall matrix is still positive definite,"}, {"start": 517.78, "end": 523.44, "text": "but now the n matrix penalizes cross products of the input and the state."}, {"start": 523.44, "end": 527.58, "text": "Now while there's uses for setting up your cost function with an n matrix, for us we're"}, {"start": 527.58, "end": 533.36, "text": "going to keep things simple and just set it to zero and focus only on q and r."}, {"start": 533.36, "end": 538.6800000000001, "text": "So by setting values of q and r, we now have a way to specify exactly what's important to"}, {"start": 538.6800000000001, "end": 539.6800000000001, "text": "us."}, {"start": 539.6800000000001, "end": 543.22, "text": "If one of the actuators is really expensive and we're trying to save energy, then we"}, {"start": 543.22, "end": 547.76, "text": "penalize it by increasing the r matrix value that corresponds with it."}, {"start": 547.76, "end": 552.04, "text": "And this might be the case if you're using thrusters for satellite control, because they use up"}, {"start": 552.04, "end": 554.66, "text": "fuel, which is a finite resource."}, {"start": 554.66, "end": 561.1, "text": "In that case, you may accept a slower reaction or more state error so that you can save fuel."}, {"start": 561.1, "end": 565.12, "text": "On the other hand, if performance is really crucial, then we can penalize state error by"}, {"start": 565.12, "end": 569.96, "text": "increasing the q matrix value that corresponds with the states that we care about."}, {"start": 569.96, "end": 573.9, "text": "And this might be the case when using reaction wheels for satellite control, because they"}, {"start": 573.9, "end": 578.64, "text": "use energy that can be stored in batteries and replenished with the solar panels."}, {"start": 578.64, "end": 584.1, "text": "So using more energy for low error control is probably a good tradeoff."}, {"start": 584.1, "end": 586.3000000000001, "text": "So now the big question."}, {"start": 586.3000000000001, "end": 589.44, "text": "How do we solve this optimization problem?"}, {"start": 589.44, "end": 593.5, "text": "I think the big disappointing answer is that deriving the solution is beyond the scope of"}, {"start": 593.5, "end": 594.58, "text": "this video."}, {"start": 594.58, "end": 597.7, "text": "But I left a good link in the description if you want to read up on it."}, {"start": 597.7, "end": 601.9000000000001, "text": "The good news, however, is that as a control system designer, often the way you approach"}, {"start": 601.9000000000001, "end": 607.7800000000001, "text": "LQR design is not by solving the optimization problem by hand, but by developing a linear model"}, {"start": 607.7800000000001, "end": 613.44, "text": "of your system dynamics, then specifying what's important by adjusting the q and r weighting matrices,"}, {"start": 613.44, "end": 619.58, "text": "then running the LQR command in MATLAB to solve the optimization problem and return the optimal"}, {"start": 619.58, "end": 625.1600000000001, "text": "gain set, and then just simulate the system and adjust q and r again if necessary."}, {"start": 625.1600000000001, "end": 630.1600000000001, "text": "So as long as you understand how q and r affects the closed loop behavior, how they punish state"}, {"start": 630.1600000000001, "end": 635.82, "text": "errors and actuator effort, and you understand that this is a quadratic optimization problem,"}, {"start": 635.82, "end": 641.58, "text": "then it's relatively simple to use the LQR command in MATLAB to find the optimal gain set."}, {"start": 641.58, "end": 647.3000000000001, "text": "Now, with LQR, we've moved the design question away from where do we place poles to the question"}, {"start": 647.3000000000001, "end": 649.3000000000001, "text": "how do we set q and r."}, {"start": 649.3000000000001, "end": 653.46, "text": "Unfortunately, there isn't a one-size-fits-all method for choosing these weights."}, {"start": 653.46, "end": 659.2800000000001, "text": "However, I'd argue that setting q and r is more intuitive than picking pole locations."}, {"start": 659.2800000000001, "end": 663.5400000000001, "text": "For example, you can just start with the identity matrix for both q and r, and then tweak them"}, {"start": 663.5400000000001, "end": 667.0, "text": "through trial and error and intuition about your system."}, {"start": 667.0, "end": 673.72, "text": "So to help you develop some of that intuition, let's walk through a few examples in MATLAB."}, {"start": 673.72, "end": 676.38, "text": "Alright, this needs a little explanation."}, {"start": 676.38, "end": 678.34, "text": "Let's start with the code."}, {"start": 678.34, "end": 682.84, "text": "I have a very simple model of a rotating mass in a frictionless environment, and the system"}, {"start": 682.84, "end": 686.18, "text": "has two states, angle and angular rate."}, {"start": 686.18, "end": 692.0, "text": "And I'm designing a full state feedback controller using LQR, and it really couldn't be simpler."}, {"start": 692.0, "end": 697.34, "text": "I'll start with the identity matrix for q, where the first diagonal entry is tied to"}, {"start": 697.34, "end": 700.72, "text": "angular error, and the second is tied to angular rate."}, {"start": 700.72, "end": 706.04, "text": "Now, there's only a single actuation input for this system, which are four rotation thrusters"}, {"start": 706.04, "end": 710.06, "text": "that all act together to create a single torque command."}, {"start": 710.06, "end": 713.14, "text": "Therefore r is just a single value."}, {"start": 713.14, "end": 718.48, "text": "Now I solve for the optimal feedback gain using the LQR command and build a state space object"}, {"start": 718.48, "end": 721.36, "text": "that represents the closed loop dynamics."}, {"start": 721.36, "end": 725.24, "text": "With the controller designed, I can simulate the response to an initial condition, which"}, {"start": 725.24, "end": 727.66, "text": "I'm setting to 3 radians."}, {"start": 727.66, "end": 729.98, "text": "And that's pretty much the whole thing."}, {"start": 729.98, "end": 733.66, "text": "Everything else in this script just makes this fancy plot so that it's easier to comprehend"}, {"start": 733.66, "end": 734.66, "text": "the results."}, {"start": 734.66, "end": 737.66, "text": "Alright, let's run this script."}, {"start": 737.66, "end": 742.32, "text": "You can see the UFO gets initialized to 3 radians, as promised."}, {"start": 742.32, "end": 746.3000000000001, "text": "And up at the top I'm keeping track of how long the maneuver takes, which is representative"}, {"start": 746.3, "end": 750.9799999999999, "text": "of the performance, and how much fuel is used to complete the maneuver."}, {"start": 750.9799999999999, "end": 754.92, "text": "So let's kick it off and see how well the controller does."}, {"start": 763.9799999999999, "end": 764.9799999999999, "text": "Hey, look at that."}, {"start": 764.9799999999999, "end": 771.2199999999999, "text": "It completed the maneuver in 5.8 seconds with 15 units of fuel, and it got the cow in the"}, {"start": 771.2199999999999, "end": 774.2199999999999, "text": "process, which is the important part."}, {"start": 774.22, "end": 779.6800000000001, "text": "When the thrusters are active, they generate a torque that accelerates the UFO over time."}, {"start": 779.6800000000001, "end": 783.6600000000001, "text": "Therefore fuel usage is proportional to the integral of acceleration."}, {"start": 783.6600000000001, "end": 787.64, "text": "So the longer we accelerate, the more fuel is used."}, {"start": 787.64, "end": 792.7, "text": "Now let's see if we can use less fuel for this maneuver by penalizing the thruster more."}, {"start": 792.7, "end": 798.1600000000001, "text": "I'll bump R up to 2 and rerun the simulation."}, {"start": 798.1600000000001, "end": 815.6400000000001, "text": "Well, we used two fewer units of fuel, but at the expense of over 3 additional seconds."}, {"start": 815.64, "end": 820.42, "text": "The problem is that with this combination, it overshot the target just a bit and had to waste"}, {"start": 820.42, "end": 822.1999999999999, "text": "time coming back."}, {"start": 822.1999999999999, "end": 826.3199999999999, "text": "So let's try to slow down the max rotation speed with the hope that it won't overshoot."}, {"start": 826.3199999999999, "end": 831.08, "text": "And we're going to do that by penalizing the angular rate portion of the Q matrix."}, {"start": 831.08, "end": 835.26, "text": "Now any non-zero rate costs double what it did before."}, {"start": 835.26, "end": 847.02, "text": "And let's give this a shot."}, {"start": 847.02, "end": 851.8199999999999, "text": "Well we saved about a second since it didn't overshoot, and in the process managed to knock"}, {"start": 851.8199999999999, "end": 854.7, "text": "another unit of fuel off."}, {"start": 854.7, "end": 856.58, "text": "Alright enough of this small stuff."}, {"start": 856.58, "end": 868.1800000000001, "text": "Let's really save fuel now by relaxing the angle error weight a bunch."}, {"start": 868.1800000000001, "end": 873.5400000000001, "text": "Okay this is going really slowly now, so let me speed up the video to just get through it."}, {"start": 873.5400000000001, "end": 877.7, "text": "In the end we used 5 units of fuel, less than half of what was used before."}, {"start": 877.7, "end": 882.58, "text": "And we can go the other way as well and tune a really aggressive controller."}, {"start": 882.58, "end": 886.5400000000001, "text": "Okay yeah, that's much faster."}, {"start": 886.5400000000001, "end": 890.0400000000001, "text": "Less than 2 seconds and our acceleration is off the charts."}, {"start": 890.0400000000001, "end": 892.58, "text": "That's how you rotate to pick up a cow."}, {"start": 892.58, "end": 898.58, "text": "Unfortunately it's at the expense of almost 100 units of fuel, so there's downsides to everything."}, {"start": 898.58, "end": 902.86, "text": "Alright, so hopefully you're starting to see how we can tweak and tune our controller by"}, {"start": 902.86, "end": 904.88, "text": "adjusting these two matrices."}, {"start": 904.88, "end": 907.4000000000001, "text": "And it's pretty simple."}, {"start": 907.4000000000001, "end": 911.46, "text": "Now I know this video is dragging on, but with a different script I want to show you one"}, {"start": 911.46, "end": 912.9000000000001, "text": "more thing real quickly."}, {"start": 912.9000000000001, "end": 917.0600000000001, "text": "And that's how LQR is more powerful than pole placement."}, {"start": 917.0600000000001, "end": 921.96, "text": "Here I have a different state space model, one that has 3 states in a single actuator."}, {"start": 921.96, "end": 925.88, "text": "And I've defined my Q and R matrices and solved for the optimal gain."}, {"start": 925.88, "end": 929.84, "text": "And like before I'll generate the closed loop state space model, and then run the response"}, {"start": 929.84, "end": 932.86, "text": "to an initial condition of 1, 0, 0."}, {"start": 932.86, "end": 938.88, "text": "I then plot the response of the first state, that step from 1 back to 0, the actuator effort,"}, {"start": 938.88, "end": 942.04, "text": "and the location of the closed loop poles and zeros."}, {"start": 942.04, "end": 946.12, "text": "So let's run this and see what happens."}, {"start": 946.12, "end": 951.7, "text": "Well the first state tracks back to 0 nicely, but at the expense of a lot of actuation."}, {"start": 951.7, "end": 956.32, "text": "Now I didn't model anything in particular, but let's say that the actuator effort is thrust"}, {"start": 956.32, "end": 957.32, "text": "required."}, {"start": 957.32, "end": 960.38, "text": "So this controller is requesting 10 units of thrust."}, {"start": 960.38, "end": 964.36, "text": "However, let's say our thruster is only capable of 2 units of thrust."}, {"start": 964.36, "end": 968.48, "text": "This controller design would saturate the thruster, and we wouldn't get the response that we're"}, {"start": 968.48, "end": 969.48, "text": "looking for."}, {"start": 969.48, "end": 973.94, "text": "Now, had we developed this controller using pole placement, the question at this point"}, {"start": 973.94, "end": 979.86, "text": "would be which of these 3 poles should we move in order to reduce the actuator effort?"}, {"start": 979.86, "end": 982.8000000000001, "text": "And that's not too intuitive, right?"}, {"start": 982.8000000000001, "end": 988.36, "text": "But with LQR we can easily go to the R matrix and penalize actuator usage by raising a single"}, {"start": 988.36, "end": 989.36, "text": "value."}, {"start": 989.36, "end": 992.1, "text": "And I'll re-run the script."}, {"start": 992.1, "end": 997.5600000000001, "text": "We see that the response is slower as expected, but the actuator is no longer saturated."}, {"start": 997.5600000000001, "end": 998.86, "text": "And check this out."}, {"start": 998.86, "end": 1004.6, "text": "All 3 closed loop poles moved with this single adjustment of R. So if we were using pole placement,"}, {"start": 1004.6, "end": 1008.76, "text": "we would have had to know how to move these poles just like this in order to reduce the"}, {"start": 1008.76, "end": 1010.36, "text": "actuator effort."}, {"start": 1010.36, "end": 1012.62, "text": "And that would be pretty tough."}, {"start": 1012.62, "end": 1015.04, "text": "So that's where I want to leave this video."}, {"start": 1015.04, "end": 1019.78, "text": "LQR control is pretty powerful, and hopefully you saw that it's simple to set up and relatively"}, {"start": 1019.78, "end": 1021.9399999999999, "text": "intuitive to tune and tweak."}, {"start": 1021.9399999999999, "end": 1027.04, "text": "And the best part is that it returns an optimal gain matrix based on how you weight, performance,"}, {"start": 1027.04, "end": 1027.96, "text": "and effort."}, {"start": 1027.96, "end": 1030.78, "text": "So it's up to you how you want your system to behave in the end."}, {"start": 1030.78, "end": 1034.56, "text": "Now if you don't want to miss the next Tech Talk video, don't forget to subscribe to this"}, {"start": 1034.56, "end": 1035.56, "text": "channel."}, {"start": 1035.56, "end": 1039.3999999999999, "text": "Also if you want to check out my channel, Control System Lectures, I cover more control theory"}, {"start": 1039.3999999999999, "end": 1040.84, "text": "topics there as well."}, {"start": 1040.84, "end": 1042.8999999999999, "text": "Thanks for watching and I'll see you next time."}]}
{
  "video_id": "wEevt2a4SKI",
  "title": "wEevt2a4SKI",
  "url": "https://www.youtube.com/watch?v=wEevt2a4SKI",
  "created_at": "2025-09-17T12:03:50.539686+00:00",
  "model": "llama3",
  "prompt_version": "topics_v1.0:19fc15e376",
  "chunking": {
    "max_duration_s": 0,
    "max_chars": 80000
  },
  "topics_mode": "title_description_with_coverage",
  "topics": [
    {
      "id": "T1",
      "title": "Introduction to LQR",
      "description": "Overview of the Linear Quadratic Regulator and its application",
      "coverage_ranges": [
        {
          "start_idx": 0,
          "end_idx": 50
        }
      ],
      "coverage_segments": [
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42,
        43,
        44,
        45,
        46,
        47,
        48,
        49,
        50
      ],
      "coverage_times": [
        {
          "start_s": 0.44,
          "end_s": 261.82
        }
      ],
      "coverage_text": "Hello everyone and welcome to another video. Today I'd like to talk about one\nof the most popular and powerful control design techniques around the linear\nquadratic regulator or LQR controller. We're gonna see that an LQR controller\nis effectively a full state feedback controller where the gain matrix K is\ncomputed in a very particular fashion. I'm gonna assume that you're a little\nbit familiar with full state feedback control but if you'd like a quick\nrefresher please check out this previous video where we introduced full state\nfeedback controllers. We're also going to see today that LQR controllers address\nsome of the practical implementation issues we encountered when designing\nfull state feedback controllers. So to get the most out of the discussion today I\nrecommend that you also check out this other previous video where we discuss\nsome of these implementation problems. Links to both of those videos are in the\ndescription below. So if all of this sounds like more fun than a clown on fire\nlet's jump over to the whiteboard and get started.\nAlright so let's talk a little bit about the linear quadratic regulator or LQR\ncontroller. Right so like we said earlier the deal with an LQR controller is that\nlet's assume that we've got a plant here it's a linear plant of our standard form\nhere but the output like we said earlier is the entire state of the system. So if\nyou have this scenario we said last time that pretty much the most powerful\ncontroller you can use here is a full state feedback controller like this.\nRight so here's your full state feedback controller. Right and the whole game plan\nat this point was to just to choose K. So last time I think we showed here that you\ncan basically try to tailor how the full state feedback controller behaves by\nchoosing desired closed-loop poles of that entire system. Right I think last time we\nsaid that you know if you had some some plant here with with certain open loop\npoles here right by the represented by these blue X's here right so these blue\nones are open loop poles. In other words it was basically eigenvalues of just the A matrix.\nRight and what we said later was that okay I can just go over to MATLAB and I can say\nplace A, B and some P desired. Right and this would give me a matrix K or it would give me the\ncontroller that would move the open loop eigenvalues in other words these blue X's to wherever I desire\nit to go here. Right where is this P desired. So we saw that by choosing locations of say\nsay I want to move them you know something like this here right where these green\nones are now my desired poles. These are my P desired. We saw that by changing these\nlocations moving them further or closer or moving only certain poles right we could\nsomehow mess with what value K was. But this was a little bit unsatisfying in the\nsense that there wasn't a real knob or a way to twist and and physically understand\nhow the changes of these green X's affect the controller or affect the behavior of\nthis overall system here. Right so that was the problem with full state feedback\ncontroller it was it was a little bit difficult to understand the relationship\nbetween P desired and the resulting K. Well this is exactly the problem that LQR is going\nto allow us to address here. What we're going to do here is try to pose this entire\nproblem here in an optimization framework. So optimization is is one of the most powerful\nbranches of mathematics. In fact I've heard a couple people say that you know it's\nit's it's probably the most important engineering topic here that that almost any\nproblem in engineering can be posed as an optimization problem if you think about it\nlong enough here. So what we should maybe do here is let's take a brief\nsidestep and talk a little bit about what is optimization and how can we maybe\napply it to this scenario. So to set the stage for the concept of optimization\nlet's talk about my daily commute. For those who don't know me I live in the\nPacific Northwest specifically I live on Bainbridge Island which is about 10 miles\nwest of Seattle Washington and I work at the University of Washington which is",
      "start_s": null,
      "end_s": null
    },
    {
      "id": "T2",
      "title": "Sensitivity Study: K-Matrix",
      "description": "Exploring the effects of changing values in the K-matrix",
      "coverage_ranges": [
        {
          "start_idx": 52,
          "end_idx": 140
        }
      ],
      "coverage_segments": [
        52,
        53,
        54,
        55,
        56,
        57,
        58,
        59,
        60,
        61,
        62,
        63,
        64,
        65,
        66,
        67,
        68,
        69,
        70,
        71,
        72,
        73,
        74,
        75,
        76,
        77,
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89,
        90,
        91,
        92,
        93,
        94,
        95,
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103,
        104,
        105,
        106,
        107,
        108,
        109,
        110,
        111,
        112,
        113,
        114,
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129,
        130,
        131,
        132,
        133,
        134,
        135,
        136,
        137,
        138,
        139,
        140
      ],
      "coverage_times": [
        {
          "start_s": 265.98,
          "end_s": 851.34
        }
      ],
      "coverage_text": "problem I'd like to consider now is how can I commute from my house to my work\nevery single day. Why don't we brainstorm some possible solutions to this problem.\nNow luckily for me the state of Washington operates the largest ferry system in the\nUnited States so one option is I can just drive from my house down to the ferry\nterminal put my car on a ferry boat go across the Sound and when I get to Seattle\njust drive the rest of the way up to the University of Washington. So let's call\nthis solution maybe Z1 where I drive from my house to the ferry catch the ferry\nand then drive to campus. What else could we do? Well if I don't want to drive maybe\nI'll catch a bus from my house to the ferry terminal walk onto the ferry boat and\nthen when I get to Seattle just find some bus that happens to be going to the UW.\nSo here that's Z2 here bus ferry bus. What about a third solution? Well if I want to get some\nexercise why don't I ride my bicycle to the ferry terminal and I can actually put my bike on the\nboat take it across the water and then when I get to Seattle ride the west of the way up to the\nUniversity of Washington. Great so Z3 bike ferry bike. Why don't we think outside the box a little bit.\nI could maybe now run from my house to the nearest shoreline of Bainbridge Island hop in the water\nand swim five miles across the Sound here and then force Gump it to campus and arrive more tired\nthan that creepy dwarf in Snow White. But you know what? Z4 where I run swim run is is actually a feasible solution.\nAlright let's just skip all of this hoo-ha and I'm just going to charter a helicopter to take me directly\nfrom my house and draw me off at my desk at the University of Washington every day. So Z5 is just charter a helicopter.\nYou know what? This sounds good. So why don't we stop here? Let's call this entire set of all possible solutions the feasible set.\nThe feasible set is basically every solution you would like to consider when solving an optimization problem.\nOkay so we saw that we had all of these potential solutions and we said we're going to call a\ncollection or a set of all of these maybe big Z and we'll just put all of these solutions in here\nand this is what's referred to as the feasible set.\nSo now that we have the feasible set put together the next thing we need is we need some way to measure\nthe goodness of each one of these items in the feasible set. This is what's known as the concept of a cost function.\nSo the idea with the cost function is that it is literally a function\nwhere you pass it in an element of the feasible set so it's one of these Z's here and all this\nthing's job is it's got one job right is it needs to evaluate how much does that item cost here okay.\nSo what will come out of this is let's call this cost J of Z here right. So again this is typically\nwe're going to use a notation J here so J of Z is going to be the cost function that measures\nhow much solution Z costs right. So this could be in terms of dollars or any other abstract metric\nas long as it's a scalar value here right where you pass it in an action and an action you know it\ncould be something complicated like all of these or it could be a vector it could be some item right\nit's some solution this thing gives you the number of dollars here right it gives you a single value\nwhich measures that right. So what we're going to see here is that the optimization problem involves\nfinding a feasible solution here that will minimize the cost of a given problem here right. So tell you\nwhat let's consider a bunch of different cases here so let me erase some of this we can get a little bit\nof space so I'd like to consider a few different scenarios here so let's look at maybe case or\nscenario one here where I want to minimize time okay. So in this case what I'd like to do is I'm going to\ndevelop a cost function let's call it J1 of Z here right where what this cost function does is it just\nmeasures the time required for action Z right. So in this context the problem here or the optimization\nproblem let's write it here as maybe P1 here right is I want to minimize J1 subject to the constraint\nhere that I'm not going to consider all possible solutions I only want to consider solutions here\nthat are in my feasible set here right only these five that we laid out here right. So if you look\nat this what the optimization problem says is I need to find the element in the feasible set which\nbasically will yield the minimum time for my commute here right. So if you look at this here right we can\nwe can iteratively by brute force just go through every single one of these and evaluate how long they\neach take um and and the winner is going to be Z5 here right you can pretty much see that out right off the bat\nthat this is going to get me from my house to campus in the minimal amount of time here. So in this\nscenario here right the thing that is optimal here is Z5 is optimal right. Okay well um how about let's\nconsider another case how about case two maybe let's underline these so we can keep them uh apart case two\nhere how about how about let's minimize money right so instead of time being important let's say I'm super\ncheap and frugal and I care about money so I need to develop a second cost function J2 of Z. This cost\nfunction's only job in life here is to measure literally the cost of action Z right. So how many\ndollars does each one of these things cost? So I can form an entirely new optimization problem let's call\nit P2 right where I now same thing I want to minimize uh this other cost function J2 here again I'm only\ngoing to look at solutions in my feasible set here right. So I go through each one of these and I ask how\nmuch does this cost here well it cost me some gas and then to catch the ferry I actually have to pay to take\nmy car across then it costs me some gas again. Oh here Z2 this is actually a little cheaper here because I don't have to pay to take my car\nacross the ferry I can just walk on which is actually um much cheaper uh oh this is this is also kind of\ncheap here but oh hey check this out Z4 this doesn't cost me a dime here right nobody's I can run for free\nto the shoreline it doesn't cost me anything to swim across well it doesn't cost me any money maybe let's\nput it that way it might cost me other things but it doesn't cost me any money to get across Puget Sound\nand I can run and oh crud Z4 the helicopter is definitely going to cost me a boatload of cash here so\nyou know what in this scenario here with this cost function the optimal thing to do is actually Z4\nright huh so this is really interesting here in the sense that this same optimization problem we can\nchange the solution by picking the cost function appropriately so tell you what let's go ahead and\nlook let's make another variation on this how about case three here let's try to look at a minimizing a\ntime and money trade-off okay so in this case why don't I make a cost function let's call it j3\nwhich is actually a combination of these two here so let's have j1 here remember j1 returned the time\nrequired and I'm going to add that to j2 which measured the cost here and what I could do here is\nI could put some scalar weights in front of these here which would allow me to sort of trade off\nbetween how much do I value time versus how much do I value money here right so in this case q and r\nare scalar let's call them weights right that we can use to tune the optimization problem here right\nso you know if you if you look at this you can basically see here that depending on on q or r you\ncan say do I care more about time or do I care more about money so just to kind of write this\ndown right if you look at this here well again maybe we should write down the problem here right\nso the problem is the same thing so p3 is now I just want to minimize this new cost function j3 of z\nsuch that subject to z is in our feasible set big z okay so depending on what values of q and r we choose\nyou probably get different values here right s …",
      "start_s": null,
      "end_s": null
    },
    {
      "id": "T3",
      "title": "State Measurement Problem",
      "description": "Discussing the issue of measuring full state and its implications on controller design",
      "coverage_ranges": [
        {
          "start_idx": 142,
          "end_idx": 230
        }
      ],
      "coverage_segments": [
        142,
        143,
        144,
        145,
        146,
        147,
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160,
        161,
        162,
        163,
        164,
        165,
        166,
        167,
        168,
        169,
        170,
        171,
        172,
        173,
        174,
        175,
        176,
        177,
        178,
        179,
        180,
        181,
        182,
        183,
        184,
        185,
        186,
        187,
        188,
        189,
        190,
        191,
        192,
        193,
        194,
        195,
        196,
        197,
        198,
        199,
        200,
        201,
        202,
        203,
        204,
        205,
        206,
        207,
        208,
        209,
        210,
        211,
        212,
        213,
        214,
        215,
        216,
        217,
        218,
        219,
        220,
        221,
        222,
        223,
        224,
        225,
        226,
        227,
        228,
        229,
        230
      ],
      "coverage_times": [
        {
          "start_s": 861.98,
          "end_s": 1443.8999999999999
        }
      ],
      "coverage_text": "have any money so any expenditure of cash is is very very bad here right so that means that r is sorry\nwhere was i put yeah r is larger than q here right so what this does is what this reverts to a minimum\nmoney problem here right so here the solution comes back to being z4 here right which makes sense\nnow where it gets really interesting here is what if you have a trade-off\nbetween q and r right in this case this is a new scenario right where now it depends it depends how\nmuch do you value time versus how much do you value money so in this case we have we have potential\nsolutions i don't know which one is going to be optimal again it depends on what values of r\nbut now you've got a z1 z2 or maybe z3 as being reasonable solutions here all right let's explore\nthis a little bit further here let's think of maybe um a case four so let's do this right here unfortunately\ni didn't manage the board space extremely well here but let's look at case four case four is basically\ni'd like to look at case three again right this interesting situation where we're trading off time\nand money but i'd like to add what are called constraints okay so what constraints allow us to do\nhere is further tailor the solution to this problem here by eliminating values in the feasible set so\nwhat this does here is again let's restate our problem here so i'm gonna i'm gonna call this p4\nnow here but again it's it's the same cost function as case three right so i want to minimize uh\nj3 right this was our trade-off cost function here subject to elements in the feasible set here\nand i'm going to tack on some additional constraints here as something like what if i say i want to make\nsome kind of requirement here that i need to get 30 minutes of exercise during my commute or more here\nso what i'm going to do is i'm going to make up a function let's call it f1 here right where f1's job\nhere is that you put in one an element out of your out of your feasible set here and what this measures\nhere is it measures the exercise time uh associated with action z right so the optimization problem\nbecomes what we saw earlier but as well i'm going to specify a constraint like i need f1 of z to be\ngreater than or equal to 30 minutes here so this is basically saying that on my commute i also want to\nmake sure that i get 30 minutes of uh exercise what this does by specifying this additional constraint\nhere is we see that it starts making some of these elements um in this original feasible set infeasible\nso for example if we look at all of these here let's let's just walk down these one at a time right\nhow much exercise am i going to get out of uh each one of these so z1 here right z1 if i pass this\nthrough my f function it's going to look and say hey you spent zero minutes exercising because you're\njust walking in the uh driving in the car then you sit on your bum on the ferry doing nothing and then\nyou drive some more so this actually does like nothing so this is now not going to be allowed any\nlonger here so z1 is no longer a feasible solution same thing with z2 you're just sitting on the bus and\nsitting on the ferry so f1 this function here or this constraint that we cooked up is going to say no\nthat's not allowed any longer okay z3 this is uh yeah actually this is still allowed because you're\ngoing to get a lot of exercise biking here on either end so this is allowed oh man z4 is definitely\nallowed right you're probably going to get more exercise than you want here and then the helicopter\nbecomes infeasible because you don't get any exercise on this helicopter here right so suddenly\nthis problem here now gets a little bit simpler because the feasible set shrunk so there's only two\nsolutions here you can either bike or uh or or run and swim here right and usually for most values of\nr and q uh q and r right you're going to see that that z4 solution that's going to take that's going to\ntake hours and hours and hours and hours so usually this this term here the the the the cost function\nthat's measuring time is going to make that uh z4 solution really unattractive so in this case here\nwhat's probably going to happen here is we're going to end up with z3 is optimal right so this is\nreally fascinating in the sense that depending on how you tailor your optimization problem right the\nsolutions change here what is considered optimal changes here right so this is actually really\ninteresting and now the question would be how can we apply this optimization framework to designing a\ncontrol system for a linear dynamic system so um let me erase the board and we'll we'll jump into that\nnext okay now that we've got that basic understanding of general optimization let's think about how we can\nset up the optimization problem for designing a linear quadratic regulator here so let's talk real quick about\nsetting up the optimization problem\nso i always think it's easiest to think about a concrete scenario so you know what if we had\nsomething like like uh like a satellite here so i'm going to try to draw a satellite here i'm obviously\nnot a very good artist i think those of you who have seen some of my other videos understand that\num but i'll just make some little cartoon here for a satellite here and what we'd like to think about\nhere is this satellite might have uh multiple states here right so the state vector for this\nsatellite might be something like um the orientation right there might be states for the for the oiler\nangles there might be states for like the position um all these other things right we will just stack\nall these up in the state vector and at the same time this aircraft this satellite might have multiple\ndifferent controls right it might have like a primary thruster here it might have smaller positioning\nthrusters or or momentum wheels or things like that here so we might want to also say that this thing\nhas multiple controls here right so this would be like the like the main thruster um you know electric thrusters\nmomentum wheels\ncontrols\netc etc etc etc right all we're getting at is that this here is a dynamic system which has\nmultiple states and multiple controls simultaneously let's go ahead and assume that this is a linear system\ngoverned by dynamics of x dot is equal to ax\nplus bu right our normal state space representation of my dynamic system basically saying that\nthe state and the control they're not free to be anything they want right there's a relationship\nbetween how the states and the controls interact and they interact through this dynamic equation here\nright so within this context here what we want to do here is let's set up an optimization problem here so\nthe optimization problem here is let's go ahead and consider a cost function\nlet's just call this thing j like we did earlier here and we're going to propose this cost function here\nit's going to be the integral from time zero to infinity of x transpose\ntimes some matrix qx plus u\ntranspose some matrix r u dt okay so here's our cost function here in this case we see that x obviously\nis your n by one state vector right u is our usual m by one control vector\nand here q is a n by n symmetric\npositive semi-definite and we'll talk about the\nuh definition of that here uh matrix here\nuh matrix here sometimes you'll see this written as q greater than zero or sometimes you might see q\nlike this kind of scripty greater than zero here um again this is a notation for this matrix is positive\nuh uh yes positive semi-definite here and finally r is a m by m symmetric positive definite\nmatrix so again sometimes written as q oh i'm sorry sorry come back to the sorry come back to the q\nhere positive semi-definite means greater than or equal to sometimes it's written like this to contrast it\nwith positive definite which means that it well we'll get to that in a second again you might see\nthis written as something strictly greater than or again this this scripty\ngreater than symbol here right okay um maybe now might be a good idea to to discuss what is the …",
      "start_s": null,
      "end_s": null
    },
    {
      "id": "T4",
      "title": "Eigenvalue-to-State Mapping with LQR",
      "description": "Using LQR to relate eigenvalues to states for better control system design",
      "coverage_ranges": [
        {
          "start_idx": 232,
          "end_idx": 310
        }
      ],
      "coverage_segments": [
        232,
        233,
        234,
        235,
        236,
        237,
        238,
        239,
        240,
        241,
        242,
        243,
        244,
        245,
        246,
        247,
        248,
        249,
        250,
        251,
        252,
        253,
        254,
        255,
        256,
        257,
        258,
        259,
        260,
        261,
        262,
        263,
        264,
        265,
        266,
        267,
        268,
        269,
        270,
        271,
        272,
        273,
        274,
        275,
        276,
        277,
        278,
        279,
        280,
        281,
        282,
        283,
        284,
        285,
        286,
        287,
        288,
        289,
        290,
        291,
        292,
        293,
        294,
        295,
        296,
        297,
        298,
        299,
        300,
        301,
        302,
        303,
        304,
        305,
        306,
        307,
        308,
        309,
        310
      ],
      "coverage_times": [
        {
          "start_s": 1448.06,
          "end_s": 1976.62
        }
      ],
      "coverage_text": "satellite here so the the concept here with uh positive semi-definite here so maybe let's write\nthat down so positive semi-definite means that this matrix q if you took it and multiplied it by some vector\nx transpose and then multiplied it again by x this is a skate going to be a scalar number here if you if\nyou do the dimensions here right this is n by one by an multiplied by an n by n multiplied by an n by one\nand excuse me when by the time you transpose this thing this becomes one by n right so you can see the\nwhole thing ends up as a one by one scalar here right this scalar quantity is always greater than or equal\nto zero to zero for all possible x values here right so that's the definition of positive semi-definite\nis that this matrix is positive semi-definite if when you compute this weird quantity x transpose qx\nit is strict it is greater than or equal to zero for any value of x this vector right this this vector\ncould have positive negative zeros whatever it doesn't matter by the time you run it through and compute\nthis you get something that is strict is greater than or equal to zero all the time here right\nsimilarly the definition of positive definite right is that again you have this this uh this this matrix\nr here which we claim is positive definite the definition is that if you take some vector u maybe\nin this case transpose times u this is strictly greater than zero for all u right so again you can\nwork out the dimensions and you see that this is a scalar number here so here's the definition of positive\nsemi-definite and positive definite and what's interesting about that is that is exactly what are the terms of\nthe integrand of the cost function here right so you see that the re the way that this cost function is\nset up here is that the integrand is always positive or uh or well actually yeah it's it's positive right\nuh for any x u combinations here right so you can never get a negative integrand the integrand is\nalways positive so let's let's keep that in the back of our head as we uh continue discussing this okay\nall right so now that we understand positive definite and positive semi definite we have a we have a rough\nkind of um feeling for what this this cost function does right you can kind of see here that again the\nintegrand is always positive and this q and this r those are sort of these weighting matrices or weighting\nvalues that we saw earlier when we were looking at our brief introduction to optimization right\nwe earlier the q and the r were somehow to trade off between time versus money here right in this case\nwe see that q and r trade off between states here non-zero states or non-zero controls we're going to\ntake a closer look at that in a second but i wanted to plant that seed in your head right now let's start\nthinking about q and r as weights uh to to determine how much we value states versus how much we value\ncontrols here right okay so now that we have the the the cost function down the overall problem now in this\nscenario here is actually tell you maybe let's let's do it over here so we can keep talking about the\nthe problem here so here's the cost function so the optimization problem let's write this again as our\nscript dp symbol here right is all i want to do now here is i want to minimize this cost function j here right\nbut um uh the controls i want to use here right there's m controls here so u is\nis a real valued uh m valued uh vector here right but we're not this is not unconstrained here there's\na certain constraint right such that the single constraint that we want to consider here is that\nagain x and u are not free to be anything you want here right it's x dot is equal to ax plus bu\nright so here is my optimization problem that i would like to solve here right and if we think\nabout this long enough what it's basically saying here is that you want to take this cost function\nhere and i want to find some control signal or some control law that is going to make this cost function\nminimal so so first of all let's stop let's think about that first ignore the constraint maybe for now\nright how would you make this minimal here so for example assume that your satellite had some non-zero\ninitial condition like x of zero was not equal to zero in other words your satellite was cocked over\nat some weird orientation and some weird position not zero right something that you that you didn't want\nhere right so this term is non-zero at time zero right now if the satellite stayed there right if you\nleft it here like let's draw a quick picture maybe of just one of the states like x1 right\nif it's here's our our initial condition right so so maybe it is banked over at 45 degrees away from\nwhere you wanted to right if you left it here right and did nothing right what does this cost function\nlook like here you can kind of see it already it's there's gonna be a square there's gonna be an x1 squared term here\nmultiply by this q let's assume that q is like all ones or something like that effectively what\nthe cost function there is measuring is it's sort of measuring the integral of this right of course\nit's going to be squared it might be skewed or weighted by that q value we're going to see that in\na second but long story short here if this stayed here at a constant value this cost function is going\nto blow up to to infinity here right because it's it's the integral from time zero to infinity here right\num so you can't have this here right this is obviously not the optimal thing to do here you\ncan't leave the satellite cocked over at 45 degrees because that yields a cost function value of infinity\nhere right so that's obviously not a good thing to do so instead what is better here right is we should\ntry to bring the system back to the origin here right and now the cost here is finite right it's just\nsort of this area under the curve is is the the cost value so this is a better solution than letting\nit stay at 45 degrees so what that means is your satellite is cocked over 45 degrees what we're gonna\ndo here is is maybe bring it back to zero that yields a better result here right however the the other flip\nside of that story here is how are you gonna bring the satellite back to zero here well you're probably\ngonna have to fire some thrusters right some positioning thrusters you're gonna have to expend\nsome amount of control in order to do that here right so if we plot again i'm this is a real rough\nshooting from the hip description of this we're gonna look at it in a more formal fashion in a second\nhere but let's just plot one of these controls like maybe u of t this is like one of the positioning\nthrusters or something like that right well what you're gonna probably have to do is is if you want to\nbring the satellite back to zero you're probably gonna have to spend some control authority to do that\nand then again you're probably gonna have to let this come back to zero here right so you're gonna\nactuate the controller and like you're gonna bring it back to zero then you're gonna stop actuating the\ncontroller right so the overall cost function you can kind of see it's sort of a combination of both\nhow long are you away from zero in the states and how much non-zero control authority did you actually\nhave to expend in order to get that here right so what you can see from this discussion here is what\nq and r do or q and r are these knobs that allow you to determine how much do you care about the state\nbeing zero versus how much do you care about the control not being zero here so let's talk about that\nreal quick in general right what you end up seeing here is that um you know roughly speaking if q is is\nbig compared to r so maybe we should write that down here right so if q is bigger let's put this in quotes\nhere than r the reason it's in quotes here is because if you can you can see q and r they're two matrices\nthey're square and they're symmetric but they're not even like the same size so how do you really\nsay one is bigger than the …",
      "start_s": null,
      "end_s": null
    },
    {
      "id": "T5",
      "title": "Neglecting Unimportant States",
      "description": "Discussing the implications of neglecting certain states in the controller design",
      "coverage_ranges": [
        {
          "start_idx": 312,
          "end_idx": 380
        }
      ],
      "coverage_segments": [
        312,
        313,
        314,
        315,
        316,
        317,
        318,
        319,
        320,
        321,
        322,
        323,
        324,
        325,
        326,
        327,
        328,
        329,
        330,
        331,
        332,
        333,
        334,
        335,
        336,
        337,
        338,
        339,
        340,
        341,
        342,
        343,
        344,
        345,
        346,
        347,
        348,
        349,
        350,
        351,
        352,
        353,
        354,
        355,
        356,
        357,
        358,
        359,
        360,
        361,
        362,
        363,
        364,
        365,
        366,
        367,
        368,
        369,
        370,
        371,
        372,
        373,
        374,
        375,
        376,
        377,
        378,
        379,
        380
      ],
      "coverage_times": [
        {
          "start_s": 1983.26,
          "end_s": 2474.62
        }
      ],
      "coverage_text": "you're basically saying here that i really care that x is not equal to zero because if so this this\nterm dominates here right so in this situation let me do this in another color here right this term dominates\nso really all you're saying here is the important thing in this case here is that if the states are\nnot equal to zero you end up with a huge cost here right who cares about what the control is it's really\nthe states that matter and drive this cost function here so in this scenario what do we end up with\nthis is basically saying here the optimal thing to do here is do what you have to but make sure those\nstates go back to zero as soon as possible here right so this is typically going to yield something\nlike um you would have a fast regulation of x to zero right and in order to do that you're going to end\nup with u is large right so this is sort of the scenario where you're saying control is cheap i don't care\nwhat you do just make sure the state goes back to zero really really quickly here right let's flip that\nand and and and and compare with you know what happens if um r uh is bigger\nthan q right so in that case we have the exact opposite scenario here right where now it's this term\nthat dominates in the cost function right so the optimal thing to do here is make sure that you\nstays as small as possible here because if if you even flinches up for or flares up if you actuate one\ncontrol just a teeny little bit this term in your cost function is going to blow up and you're going to\nend up with a huge cost here right so the intent under this situation the right thing to do here is\nuse control very very very sparingly here right and who cares what the state ends up doing in this\nscenario if it stays if the satellite stays cocked over and just takes you know hundreds of seconds to\ncome back to to the origin that's fine as long as you don't fire those thrusters because the propellant\nyou're expending is is extremely valuable here right so the behavior that typically ends up happening here\nis you know if you have r bigger than q here you have very slow uh regulation of x to the origin here\nright and u is small right so this is really interesting here so you end up with this the the\nthe first case here this is like an aggressive controller right and down here you have a conservative\ncontroller so we see that q and r are these knobs that are going to allow us to to to tweak the\nstates and the controls or or or excuse me tweak the controller depending on how much we care about\nthe states and controls let's look at a very concrete example the cost function and i think we'll be able\nto see that here so for example here right let's look at a a two state two control system\nright so what if your satellite here that we're looking at it only had two states like uh like we\nwere looking uh at just the um sorry let me just write this down so i can i can talk at the same time\nhere right there's only two states like maybe i only care about the pitch angle and the pitch rate or\nsomething like that of it and then i only have two controls here like i have a electric thruster and a\nsingle momentum wheel or something like that right okay and now let's let's pick values of this q and\nthese r matrices here so in other words let's first start with something really really simple let's make\nthe q matrix just diagonal and we'll have a q11 and a q1 uh sorry q22 let's make zeros over here\nsimilarly for the r matrix right r is just going to be an r1100 r22 right okay so these are these are\nthese are symmetric and as long as all of these these entries are positive we'll see that these\nyield a positive semi-definite and a positive uh definite matrix here right so if this is the case\nlet's compute this cost function here or more importantly maybe let's just compute the integrand\nof the cost function here um maybe let's see what can we do yeah yeah let's let's erase this i think we\ngot the got the picture here okay so now what i want to do here is our is compute the integrand here\nright of x transpose q x plus u transpose ru right so run to mathematica and plug all this in here and\nwhat you'll end up seeing here is that this integrand looks like it's it's pretty simple it's a q11x1\nsquared plus a q22x2 squared maybe let's put the squared like this sorry um plus an r11u1\nsquared plus an r22u2 squared here right so here i think you can very clearly explicitly see\nwhat do these q11q2 and 22 and r11 and r22 entries uh physically mean right if you look at this right\nwe see that q11q1 is sort of the uh the weight or the the penalty effectively on a non-zero x1 right\nbecause it shows up right here right so if x1 is not zero right um if it's positive or negative by the\ntime you square it this term is going to be positive here right so as long as the magnitude is not zero this\nsquare term is something uh positive and q11 is now this this multiplicative factor here showing you\nhow much does it matter that x1 is not zero here right it will either exacerbate that in terms of\nthe cost or it would attenuate it depending on on this value here right and similarly let's just go\nthrough all of these just to be very very explicit so q22 right this is the weight or the penalty on a\nnon-zero x2 right so we very clearly see that the q matrix by choosing the entries appropriately you can\nnow say which state do i really care about being non-zero right i can either say that i really care\nabout x1 being non-zero or i care about x2 being not zero similarly the r11 term right you see it right\nhere right this is the weight or the penalty on a non-zero u1 of t right and similarly r22 is just the\nweight or the penalty on a non-zero u2 of t right so r11 is basically it's it's actually telling you\npretty much exactly how expensive is it to fire or to to utilize control number one right yeah this was\nlike maybe your your electric thruster maybe it doesn't cost you anything at all right because\nthat's the whole deal with electric thrusters is that electric thrusters are are are cheap um they\nare long lasting they're reliable here so so if you want to model that fact here in your control design\nyou can maybe pick a pick a small r11 right because if r11 is small it's basically saying i don't care\nwhat you're doing with this controller this controller is super duper cheap because even though if this\ncontrol is is is large by the time i multiply by a small r11 this doesn't contribute to the overall\ncost function much right and in the same fashion you see that r22 is is the is the cost of actuating\ncontroller number two here right so that's pretty awesome this i think allows us to set up the entire\noptimization problem here right now the thing that's interesting here is now if we start coming back and\nthinking about um the the constraint here right this is the interesting part we i think we understand\nthe cost function at this point right we understand how to tailor or to to tune this cost function using",
      "start_s": null,
      "end_s": null
    },
    {
      "id": "T6",
      "title": "LQR Summary and Future Directions",
      "description": "Recap of LQR concepts and hints at future topics to be covered",
      "coverage_ranges": [
        {
          "start_idx": 382,
          "end_idx": 450
        }
      ],
      "coverage_segments": [
        382,
        383,
        384,
        385,
        386,
        387,
        388,
        389,
        390,
        391,
        392,
        393,
        394,
        395,
        396,
        397,
        398,
        399,
        400,
        401,
        402,
        403,
        404,
        405,
        406,
        407,
        408,
        409,
        410,
        411,
        412,
        413,
        414,
        415,
        416,
        417,
        418,
        419,
        420,
        421,
        422,
        423,
        424,
        425,
        426,
        427,
        428,
        429,
        430,
        431,
        432,
        433,
        434,
        435,
        436,
        437,
        438,
        439,
        440,
        441,
        442,
        443,
        444,
        445,
        446,
        447,
        448,
        449,
        450
      ],
      "coverage_times": [
        {
          "start_s": 2480.7,
          "end_s": 2937.82
        }
      ],
      "coverage_text": "problem now here subject to the linear dynamics of this system here right so that's where it gets\ninteresting so why don't i pause the video i'll erase the board and now let's talk about after we set up the\noptimization problem how can we solve this optimization problem okay so now what we'd like to do here is\nlook at solving the optimization problem so graphically what this means here is again here we've got our\nlinear plant right which is governed by x dot is equal to ax plus bu\nright and now what i want to do is i want to find a control law u here\nwhich is going to make this whole system here um optimal here right so i'm going to choose a u here\nso that the x uh by the time i look at the combination of u and x in that cost function it stays small\naccording to the cost function that we set up here right so um it turns out that the solution to our\nproblem here so i'll just write this down here so the solution to our optimization problem here right\nwhich was we said earlier minimize this cost function of integral zero to infinity of x transpose qx\nplus u transpose r u dt uh subject to these dynamics here right\nthis was the optimization problem here well some really smart people figured it out it's beyond the\nscope of this lecture today to derive it here but i think we can use their results here the solution here\nis that you better choose a control here that looks like this minus k x of t where this gain k here is given by\nr inverse b transpose s here and s\nis the solution\nto what's called the algebraic ricotti\nequation of this so a\ntranspose s plus s a minus s b r inverse or r inverse b transpose s plus q equals zero here right\nokay great um and here yeah s is the solution algebraic ricotti equation and also it is\nit's size n by n and square and symmetric\nokay so this is the solution here right so tell you what if we first if we first ignore everything\nbelow here just look at look at the solution here right the solution is minus kx we've seen that before\nthis is just a full state feedback\ncontroller\nright so in other words the architecture that goes along with this is that this thing just looks\nlike\nminus k\nright so the optimal thing to do the absolute best thing you can do to solve this problem is to use a\nfull state feedback controller here now what gets interesting here is that the way you get this k here\nyou have to go through a couple of steps here right if you look at this so step one here is we got to\nsolve the algebraic ricotti equation for s here so let's talk about this the so the procedure\nfor lqr right so step one here right we are going to be given\nour a and b matrix right those come from the plan right those are known right okay they're known and\nimmutable you can't really change them the next thing we have to do here is you you are going to\npick the q and the r right because you as a control designer you get to choose how much do i care about\nstates versus how much do i care about control um actuation here right so you choose the the cost\nfunction effectively here right so um let's say step two is choose q and r right you're basically\ndesigning the cost function now according to this step three here is once i have q and r i need to\nsolve the algebraic ricotti equation for s here right so solve i'm just going to abbreviate this as a\nr e here right so this down here is the algebraic ricotti equation here right okay so you solve the\nalgebraic ricotti equation for s all right what you're going to end up seeing here is that once\nyou solve for s there's actually going to be multiple solutions to s we'll take a look at this\nin a second here but if you follow a couple of steps you'll be able to find out which is the one\nyou want here once you know that here um well actually no i take that back let's just say solve\nthis thing for s you're going to get multiple solutions therefore you can then compute k you're\ngoing to get multiple solutions for k here right so you can go ahead and compute your optimal gain k\nusing our expression over there right r inverse b transfer transfer transpose s here right and again\nsince there are multiple solutions of s you'll get multiple solutions for k here the last thing that\nyou have to do then is typically there's only going to be one solution here that yields a stable situation\nhere so um if you remember earlier with this control of a full state feedback here right this\nthing is basically going to behave at the end of the day like x dot is equal to a minus bk x right and\nthis here is like maybe you're you're a closed loop um uh a matrix here right again if if this is if this\nis brand new to you here maybe take a quick refresher here to our video where we discuss full state feedback\ncontrollers but we show that under this control law the dynamics reduced to this here right now what\nyou can do here is you've got multiple k's here right so that means you have multiple a closed loops\nyou can evaluate each of these eight closed loops and find its eigenvalues and usually there's only one\nof them that are going to have all poles in the uh or eigenvalues in the left half plane here so\nlet me let's say step five here is choose um the case solution that yields\na stable system\nyeah so that's the workflow that we need to go through so uh again i love concrete examples let's\nlook at a concrete example right now to try to implement all this so the example i'd like to use\nhere is a really simplified scenario it's a um a mass and a damper that's it okay so let's look at an\nexample here of a mass uh slash damper system so you got some smooth surface like i don't know like\nsome ice or something like that you got a mass sitting on this thing and all we're going to be doing\nhere is let's consider the position and the velocity here like such positive position and positive\nvelocity is to the right here and let's say that you have um a single control of this you're able to\nexert some kind of force on this block so you maybe have one rocket engine or one thruster that pushes",
      "start_s": null,
      "end_s": null
    }
  ],
  "llm_raw": {
    "mode": "whole_simple",
    "attempts": [
      "Here is the table of contents in the required format:\n\n```\n[\n  {\n    \"title\": \"Introduction to LQR\",\n    \"description\": \"Overview of the Linear Quadratic Regulator and its application\",\n    \"coverage\": [\n      {\"start_idx\": 0, \"end_idx\": 50}\n    ]\n  },\n  {\n    \"title\": \"Sensitivity Study: K-Matrix\",\n    \"description\": \"Exploring the effects of changing values in the K-matrix\",\n    \"coverage\": [\n      {\"start_idx\": 52, \"end_idx\": 140}\n    ]\n  },\n  {\n    \"title\": \"State Measurement Problem\",\n    \"description\": \"Discussing the issue of measuring full state and its implications on controller design\",\n    \"coverage\": [\n      {\"start_idx\": 142, \"end_idx\": 230}\n    ]\n  },\n  {\n    \"title\": \"Eigenvalue-to-State Mapping with LQR\",\n    \"description\": \"Using LQR to relate eigenvalues to states for better control system design\",\n    \"coverage\": [\n      {\"start_idx\": 232, \"end_idx\": 310}\n    ]\n  },\n  {\n    \"title\": \"Neglecting Unimportant States\",\n    \"description\": \"Discussing the implications of neglecting certain states in the controller design\",\n    \"coverage\": [\n      {\"start_idx\": 312, \"end_idx\": 380}\n    ]\n  },\n  {\n    \"title\": \"LQR Summary and Future Directions\",\n    \"description\": \"Recap of LQR concepts and hints at future topics to be covered\",\n    \"coverage\": [\n      {\"start_idx\": 382, \"end_idx\": 450}\n    ]\n  }\n]\n```"
    ]
  }
}
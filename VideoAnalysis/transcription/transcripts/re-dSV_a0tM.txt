re-dSV_a0tM
https://www.youtube.com/watch?v=re-dSV_a0tM
Unknown Category
Marvin, turn on the lights. Okay. Marvin, turn off the bedroom. Okay. Marvin, turn off the kitchen. Okay. Marvin, tell me a joke. What goes up and down but does not move? Stairs. Marvin, turn off the lights. Okay. Hey everyone. So, if you've been playing along at home, you'll have known that we've been building towards something. We've covered getting audio into the ESP32, getting audio out of the ESP32, and we've looked at getting some AI running using TensorFlow Lite. This has all been building towards building an Alexa type system. So what actually is an Alexa system? What components do we need to plug together to get something working? So, the first thing we're going to need is some kind of wake word detection system. This will continuously listen to audio waiting for a trigger phrase or word. When it hears this word, it will wake up the rest of the system and start recording audio to capture whatever instructions the user has. Once the audio has been captured, it will send it off to a server to be recognised. The server processes the audio and works out what the user is asking for. The server may process the user's request and may trigger actions in other services. In the system we're building, we'll just be using the server to work out what the user's intention was. This intention is then sent back to the device and the device tries to perform what the user asked it to do. So we need three components. A wake word detection. Audio capture and intent recognition. And intent execution. Let's start off with the wake word detection. We're going to be using TensorFlow Lite for our wake word detection. And as with any machine learning problem, our first port of call is to find some data to train against. Now fortunately the good folk at Google have already done the heavy lifting for us and collated a speech commands dataset. So this dataset contains over 100,000 audio files consisting of a set of 20 core command words such as up, down, left, right, yes, no and a set of extra words. Each of the samples is one second long. There's one word in particular that looks like a good candidate for a wake word. I've chosen to use the word Marvin as my wake word. Oh god, I'm so depressed. Let's have a listen to a couple of the files. Marvin Marvin Marvin Marvin Marvin Seven Seven Seven Seven I've also recorded a large sample of ambient background noise consisting of TV, radio shows and general office noise. So now we've got our training data we need to work out what features to train our neural network against. It's not likely that feeding in raw audio samples will give us a good result. So reading around and looking at some TensorFlow samples A good approach seems to be to treat the problem as an image recognition problem. We need to turn our audio samples into something that looks like an image. To do this we can take a spectrogram of the audio sample. To get a spectrum of an audio sample we break the sample into small sections. We then perform a discrete Fourier transform on each of these sections. This gives us the frequencies that are present in that slice of audio. Putting these frequency slices together gives us a spectrogram of the sample. Now I've created a Jupyter notebook to create the training data. As always the first thing we do is import the libraries we're going to need and set up some constants. We've got a list of words in our training data along with a dummy word for the background noise. I've made some helper functions for getting all the files for a word and also for detecting if the file actually contains voice data. Some of the samples are not exactly one second long and some of them have truncated audio data. We then have our function for generating the spectrogram for an audio sample. We first make sure the audio sample is normalised and then we compute the spectrogram. We reduce the result of this by applying average pooling. We finally take a log of the spectrogram so that we don't feed extreme values into our neural network which might make it harder to train. For each file we collect training data from we apply some random modifications. We randomly shift the audio sample in its one second segment. This makes sure that our neural network generalises around the audio position. We also add in some random sample of background noise. This helps our neural network work out the unique features of our target word and ignore any background noise. Now we need to add more samples of the Marvin word to our dataset as it would otherwise be swamped by the other words in our training data. So we repeat it multiple times. This also helps our neural network generalise as there will be multiple samples of the word different background noise and in different positions in the one second sample. We then add in samples from our background noise. We run through each file of background noise and chop it into one second segments. And then we also generate some random utterances from our background noise. Once again this should help our network distinguish between the word Marvin and random noises. Now during my testing of the system I found that there were some particular noises that seem to trigger false detection of the word Marvin. These seem to consist of low frequency humming and strange scraping sounds. So I've collected some of these sounds as more negative samples for the training process. With all this data we end up with a reasonably sized training validation and testing dataset. So we can save this to disk for use in our training workbook. We can also have a look at the spectrograms for different words in our training data. So here's some examples of Marvin. And here's some examples of the word yes. So that's our training data prepared. Let's have a look at how we train our model up. I've created another Jupyter notebook for training our model. Once again we have to pull in the imports we need. We also set up TensorBoard so that we can visualise the training of our model. We've got our list of words. It's important that this is in the same order as in the training workbook. And we have the code to load up our training data. If we plot a histogram of the training data you can see that we have a lot of examples of the word at position 16 and quite a few at position 35. Combining this with our words we can see that this matches up to the word Marvin and to our background noise. Now for our system we only really care about detecting the word Marvin so we'll modify our y labels so that it contains a 1 for Marvin and a 0 for everything else. Plotting another histogram we can see that we now have a fairly balanced set of training data with examples of our positive and negative classes. We can now feed our data into TensorFlow datasets. We set up our training data to repeat forever, randomly shuffle and to come out in batches. Now we create our model. I've played around with a few different model architectures and ended up with this as a trade-off between time to train, accuracy and model size. We have a convolution layer followed by a max pooling layer. We have a convolution layer followed by another convolution layer with a max pooling layer and the result of this is fed into a densely connected layer and finally to our output neuron. Looking at the summary of our model we can see how many parameters it has. This gives us a fairly good indication of how large the model will be when we convert it to TensorFlow Lite. Finally, we compile our model, set up the tensorboard logging and kick off the training. With our training completed we can now take a look at how well it has performed. Looking at the tensorboard we can see that our training performance is pretty close to our validation performance. There is a bit of noise on the unsmoothed lines. Ideally we should probably try and increase the size of our training and validation data. Let's see how well it does on the testing dataset. I'm going to use the best model that was found during training and work from that. You can see that we get pretty good results. Checking the confusion matrix we can see how many false positives and how many false negatives we get. These are pretty good results as well. I would rather get more false positives than false negatives as we don't want to be randomly waking up from background noises. Let's try it with a higher threshold and see how that performs. This is probably what we will go for in our code. We will get a lot more false negatives but also far fewer false positives. So as we don't seem to be overfitting I am happy to train the model on our complete dataset. We will get a lot of training, validation and testing all combined into one large dataset. Let's see how this performs on all our data. Once again we have pretty good results. Our next step is to convert the model to TensorFlow Lite for use on the ESP32. So let's jump into another workbook for this. We have our imports to bring in TensorFlow and NumPy. We are also going to need our data. We need this so that the converter can quantize our model accurately. Once the model has been converted we can run a command line tool to generate the C code. We can now compile that into our project. We'll take a look at the wake word detection code on the ESP side of things later. Next we need to get to another building block. Once we've detected the wake word we need to record some audio and work out what the user wants us to do. We're going to need something that can understand speech. So to do the heavy lifting of actually recognising the text we're going to be using a service from Facebook called Wit AI. So this is a free service that will analyse speech and work out what the intention is behind the speech. So we'll log in using Facebook. And then the first thing we need to do is create a new application. So let's just call this marty1. We'll make it private for now. And we'll create. So now we need to train our application to work out what it is we're trying to do. So let's add a few sample phrases. So let's try turning something on. So we need to create an intent. And then we need to start highlighting some of the bits of text. So let's try and pull out the device that we're trying to turn on. So I'll create an entity. So now we have an entity called device. And we've highlighted bedroom as the piece of text that should correspond to that device. And now we can add a trait for on and off. So this is a built-in trait that's supplied by Wit. And we want to say that this should be turned on. So let's train this. And now let's try adding another piece of text. So you can see that it's worked out the device already. And it's worked out the value should be off. So let's add that to our turn off and on intent. Let's try adding another one. So let's try turn on the kitchen. So it's worked out that it's an on-off trait. And it's worked out on. And then let's highlight this and tell it that's the device. And we'll train that as well. So let's try another one. Turn off the kitchen. So it's improved its understanding. Now it can see the device is kitchen and the trait is off. So let's try and validate that. Now you can keep adding more utterances to improve the performance of your application. But I think for now that should be enough for our use case. So let's try this out with some real text. So I've made some sample utterances and recorded them to WAV files. So I have a turn off, a turn on, and another example turn on. So let's have a quick listen to these files. So turn off. Turn off the bedroom. So hopefully that should turn off the bedroom. So let's try running this through wit. So I have a curl command here that will post up the WAV file to the backend service. So you can see it's detected that the device is bedroom. And it's detected that we want to turn off the bedroom. So let's try another one. So we'll try turn on. Turn on the light. So this should turn on the light. So let's try that. So we can see once again it's detected the device. It tells us that it's the light. It's found the intent, turn on device. But it says we want to turn it on. So let's try our last sample. So turn on two. Turn on bedroom. So this should turn on the bedroom as well. So let's check that works. So yeah. So it's found the device and it's worked out. We want to turn it on. So I think this wit application should work for us. So let's integrate it into our code. So that's our building blocks completed. We have something that will detect a wake word. And we have something that will work out what the user's intention was. Let's have a look at how this is all wired up on the ESP32 side of things. I've created a set of libraries for the main components of the project. We have the TFMicro library which includes everything needed to run a TensorFlow Lite model. And we have a wrapper library to make it slightly easier to use. Here's our trained model converted into C code. And here are the functions that we'll use to communicate with it. We have one to get the input buffer and another to run a prediction on the input data. We've covered this in more detail in a previous video so I won't go into too many details on this now. Moving on, we have a couple of helper libraries for getting audio in and out of the system. We can support both I2S microphones directly and analogue microphones using the analogue to digital converter. Samples from the microphone are heading to a circular buffer with room for just over 1 seconds worth of audio. Our audio output library supports playing WAV files from SPIFs via an I2S amplifier. We've then got our audio processing code. This needs to recreate the same process that we used for our training data. The first thing we need to do is work out the mean and max values of the samples so that we can normalise the audio. We then step through the 1 second of audio, extracting a window of samples on each step. The input samples are normalised and copied into our FFT input buffer. Now the input to the FFT is a power of 2 so there is a blank area that we need to zero out. Before performing the FFT we apply a hamming window and then once we have done the FFT we extract the energy in each frequency bin. We follow that by the same average pooling process as in training and then finally we take the log. This gives us the set of features that our neural network is expecting to see. Finally we have the code for talking to WIT AI. To avoid having to buffer the entire audio sample in memory we need to perform a chunked upload of the data. We create a connection to WIT AI and then upload the chunks of data until we have collected sufficient audio to capture the user's command. We decode the results from WIT AI and extract the pieces of information that we are interested in. We only care about the intent, the device and whether the user wants to turn the device on or off. That's all the components of our application. Let's see how these are all coordinated. In our setup function we do all the normal work of setting up the serial port, connecting to WiFi and starting up SPFs. We configure the audio input and the audio output and we set up some devices and map them onto GPIO ports. Finally we create a task that will delegate onto our application class before we kick off the audio input. Our application task is woken up every time the audio input fills up one of the sections of the ring buffer. Every time that happens it services the application. Our application consists of a very simple state machine. We can be in one of two states. We can either be waiting for the wake word or we can be recognising a command. Let's have a look at the detect wake word state. The first thing we do is get hold of the ring buffer. We rewind by one seconds worth of samples and then generate the spectrogram. This spectrogram is fed directly into the neural network's input buffer so we can run the prediction. If the neural network thinks the wake word occurred then we move onto the next state. Otherwise we stay in the current state. For the command recognition state. When we enter the state we make a connection to witai. This can take up to 1.5 seconds as making an SSL connection on the ESP32 is quite slow. We then start streaming samples to the server. To allow for the SSL connection time we go back one second into the past so we don't miss too much of what the user said. Once we streamed 3 seconds of samples we ask witai what the user said. We could be more clever here and we could wait until we think the user has stopped speaking but that's probably work for a future version. Witai processes the audio and tells us what the user asked. We pass that onto our intent processor to interpret the request and move onto the next state which will put us back into waiting for the wake word. Our intent processor simply looks at the intent name that witai provides us and carries out the appropriate action. the user's intent to interpret the request and the user's intent. Marvin, tell me about life. LIFE. Don't talk to me about LIFE. So there we have it. A DIY Alexa. How well does it actually work? It works reasonably well. We have a very lightweight wake word detection system. It runs in around 100ms and there's still room for lots of optimisation. Accuracy on the wake word is ok. We do need more training data to make it really robust. You can easily trick it into activating by using similar words to Marvin such as marvellous, Martin, Marlin. More negative examples of words would help with this problem. The witai system works very well and you can easily add your own intents and traits and build a very powerful system. There are also alternative paid versions which you can use instead. One is available from Microsoft and Google and Amazon also have similar and equivalent services. All the code is in GitHub. The link is in the description. All you actually need is a microphone to get audio data into the ESP32. You don't necessarily need a speaker. You can just comment out the sections that try and talk to you. Let me know how you get on in the comments section. that you are typing shows a 81 that it would also be making some very smart fellow Casting it to the AVadads. If you had that sensitivity around,aremisation is a good score. I'm always sad that you can do längers with your neon head, which are completely incredible. You should know how to check out the effects and the other things that have displayed all. Lots of things they make different. When you are goog Old and intents andendo and learning, learn about which Phi. The most experimental應 ARgs are not medio how you can use some estuv Casey. That skill is particularly really important to you.
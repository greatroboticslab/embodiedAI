wEevt2a4SKI
https://www.youtube.com/watch?v=wEevt2a4SKI
Unknown Category
Hello everyone and welcome to another video. Today I'd like to talk about one of the most popular and powerful control design techniques around the linear quadratic regulator or LQR controller. We're gonna see that an LQR controller is effectively a full state feedback controller where the gain matrix K is computed in a very particular fashion. I'm gonna assume that you're a little bit familiar with full state feedback control but if you'd like a quick refresher please check out this previous video where we introduced full state feedback controllers. We're also going to see today that LQR controllers address some of the practical implementation issues we encountered when designing full state feedback controllers. So to get the most out of the discussion today I recommend that you also check out this other previous video where we discuss some of these implementation problems. Links to both of those videos are in the description below. So if all of this sounds like more fun than a clown on fire let's jump over to the whiteboard and get started. Alright so let's talk a little bit about the linear quadratic regulator or LQR controller. Right so like we said earlier the deal with an LQR controller is that let's assume that we've got a plant here it's a linear plant of our standard form here but the output like we said earlier is the entire state of the system. So if you have this scenario we said last time that pretty much the most powerful controller you can use here is a full state feedback controller like this. Right so here's your full state feedback controller. Right and the whole game plan at this point was to just to choose K. So last time I think we showed here that you can basically try to tailor how the full state feedback controller behaves by choosing desired closed-loop poles of that entire system. Right I think last time we said that you know if you had some some plant here with with certain open loop poles here right by the represented by these blue X's here right so these blue ones are open loop poles. In other words it was basically eigenvalues of just the A matrix. Right and what we said later was that okay I can just go over to MATLAB and I can say place A, B and some P desired. Right and this would give me a matrix K or it would give me the controller that would move the open loop eigenvalues in other words these blue X's to wherever I desire it to go here. Right where is this P desired. So we saw that by choosing locations of say say I want to move them you know something like this here right where these green ones are now my desired poles. These are my P desired. We saw that by changing these locations moving them further or closer or moving only certain poles right we could somehow mess with what value K was. But this was a little bit unsatisfying in the sense that there wasn't a real knob or a way to twist and and physically understand how the changes of these green X's affect the controller or affect the behavior of this overall system here. Right so that was the problem with full state feedback controller it was it was a little bit difficult to understand the relationship between P desired and the resulting K. Well this is exactly the problem that LQR is going to allow us to address here. What we're going to do here is try to pose this entire problem here in an optimization framework. So optimization is is one of the most powerful branches of mathematics. In fact I've heard a couple people say that you know it's it's it's probably the most important engineering topic here that that almost any problem in engineering can be posed as an optimization problem if you think about it long enough here. So what we should maybe do here is let's take a brief sidestep and talk a little bit about what is optimization and how can we maybe apply it to this scenario. So to set the stage for the concept of optimization let's talk about my daily commute. For those who don't know me I live in the Pacific Northwest specifically I live on Bainbridge Island which is about 10 miles west of Seattle Washington and I work at the University of Washington which is actually on the other side of this large body of water called Puget Sound. So the problem I'd like to consider now is how can I commute from my house to my work every single day. Why don't we brainstorm some possible solutions to this problem. Now luckily for me the state of Washington operates the largest ferry system in the United States so one option is I can just drive from my house down to the ferry terminal put my car on a ferry boat go across the Sound and when I get to Seattle just drive the rest of the way up to the University of Washington. So let's call this solution maybe Z1 where I drive from my house to the ferry catch the ferry and then drive to campus. What else could we do? Well if I don't want to drive maybe I'll catch a bus from my house to the ferry terminal walk onto the ferry boat and then when I get to Seattle just find some bus that happens to be going to the UW. So here that's Z2 here bus ferry bus. What about a third solution? Well if I want to get some exercise why don't I ride my bicycle to the ferry terminal and I can actually put my bike on the boat take it across the water and then when I get to Seattle ride the west of the way up to the University of Washington. Great so Z3 bike ferry bike. Why don't we think outside the box a little bit. I could maybe now run from my house to the nearest shoreline of Bainbridge Island hop in the water and swim five miles across the Sound here and then force Gump it to campus and arrive more tired than that creepy dwarf in Snow White. But you know what? Z4 where I run swim run is is actually a feasible solution. Alright let's just skip all of this hoo-ha and I'm just going to charter a helicopter to take me directly from my house and draw me off at my desk at the University of Washington every day. So Z5 is just charter a helicopter. You know what? This sounds good. So why don't we stop here? Let's call this entire set of all possible solutions the feasible set. The feasible set is basically every solution you would like to consider when solving an optimization problem. Okay so we saw that we had all of these potential solutions and we said we're going to call a collection or a set of all of these maybe big Z and we'll just put all of these solutions in here and this is what's referred to as the feasible set. So now that we have the feasible set put together the next thing we need is we need some way to measure the goodness of each one of these items in the feasible set. This is what's known as the concept of a cost function. So the idea with the cost function is that it is literally a function where you pass it in an element of the feasible set so it's one of these Z's here and all this thing's job is it's got one job right is it needs to evaluate how much does that item cost here okay. So what will come out of this is let's call this cost J of Z here right. So again this is typically we're going to use a notation J here so J of Z is going to be the cost function that measures how much solution Z costs right. So this could be in terms of dollars or any other abstract metric as long as it's a scalar value here right where you pass it in an action and an action you know it could be something complicated like all of these or it could be a vector it could be some item right it's some solution this thing gives you the number of dollars here right it gives you a single value which measures that right. So what we're going to see here is that the optimization problem involves finding a feasible solution here that will minimize the cost of a given problem here right. So tell you what let's consider a bunch of different cases here so let me erase some of this we can get a little bit of space so I'd like to consider a few different scenarios here so let's look at maybe case or scenario one here where I want to minimize time okay. So in this case what I'd like to do is I'm going to develop a cost function let's call it J1 of Z here right where what this cost function does is it just measures the time required for action Z right. So in this context the problem here or the optimization problem let's write it here as maybe P1 here right is I want to minimize J1 subject to the constraint here that I'm not going to consider all possible solutions I only want to consider solutions here that are in my feasible set here right only these five that we laid out here right. So if you look at this what the optimization problem says is I need to find the element in the feasible set which basically will yield the minimum time for my commute here right. So if you look at this here right we can we can iteratively by brute force just go through every single one of these and evaluate how long they each take um and and the winner is going to be Z5 here right you can pretty much see that out right off the bat that this is going to get me from my house to campus in the minimal amount of time here. So in this scenario here right the thing that is optimal here is Z5 is optimal right. Okay well um how about let's consider another case how about case two maybe let's underline these so we can keep them uh apart case two here how about how about let's minimize money right so instead of time being important let's say I'm super cheap and frugal and I care about money so I need to develop a second cost function J2 of Z. This cost function's only job in life here is to measure literally the cost of action Z right. So how many dollars does each one of these things cost? So I can form an entirely new optimization problem let's call it P2 right where I now same thing I want to minimize uh this other cost function J2 here again I'm only going to look at solutions in my feasible set here right. So I go through each one of these and I ask how much does this cost here well it cost me some gas and then to catch the ferry I actually have to pay to take my car across then it costs me some gas again. Oh here Z2 this is actually a little cheaper here because I don't have to pay to take my car across the ferry I can just walk on which is actually um much cheaper uh oh this is this is also kind of cheap here but oh hey check this out Z4 this doesn't cost me a dime here right nobody's I can run for free to the shoreline it doesn't cost me anything to swim across well it doesn't cost me any money maybe let's put it that way it might cost me other things but it doesn't cost me any money to get across Puget Sound and I can run and oh crud Z4 the helicopter is definitely going to cost me a boatload of cash here so you know what in this scenario here with this cost function the optimal thing to do is actually Z4 right huh so this is really interesting here in the sense that this same optimization problem we can change the solution by picking the cost function appropriately so tell you what let's go ahead and look let's make another variation on this how about case three here let's try to look at a minimizing a time and money trade-off okay so in this case why don't I make a cost function let's call it j3 which is actually a combination of these two here so let's have j1 here remember j1 returned the time required and I'm going to add that to j2 which measured the cost here and what I could do here is I could put some scalar weights in front of these here which would allow me to sort of trade off between how much do I value time versus how much do I value money here right so in this case q and r are scalar let's call them weights right that we can use to tune the optimization problem here right so you know if you if you look at this you can basically see here that depending on on q or r you can say do I care more about time or do I care more about money so just to kind of write this down right if you look at this here well again maybe we should write down the problem here right so the problem is the same thing so p3 is now I just want to minimize this new cost function j3 of z such that subject to z is in our feasible set big z okay so depending on what values of q and r we choose you probably get different values here right so in general right you can see here that a large q relative to r right what is that going to do here right that's basically saying that you're reverting to a a minimum time problem right because if q is big then if the time is big this b the the whole entire cost function becomes big here so you really want to um uh choose solutions that minimize time here so if you do this here this is basically saying you uh whoops i should be relative to r right you're you're super rich and your time is more important than your money here right so if you do this um you say what we're basically doing here is this this this kind of reverts uh to a min time problem right basically reverts to this case here right so we see that all right in this case you end up again back with with our z5 um solution of just getting the helicopter right alternatively if you have a small q relative to r right this is now saying that uh no actually uh you're super frugal right you don't have any money so any expenditure of cash is is very very bad here right so that means that r is sorry where was i put yeah r is larger than q here right so what this does is what this reverts to a minimum money problem here right so here the solution comes back to being z4 here right which makes sense now where it gets really interesting here is what if you have a trade-off between q and r right in this case this is a new scenario right where now it depends it depends how much do you value time versus how much do you value money so in this case we have we have potential solutions i don't know which one is going to be optimal again it depends on what values of r but now you've got a z1 z2 or maybe z3 as being reasonable solutions here all right let's explore this a little bit further here let's think of maybe um a case four so let's do this right here unfortunately i didn't manage the board space extremely well here but let's look at case four case four is basically i'd like to look at case three again right this interesting situation where we're trading off time and money but i'd like to add what are called constraints okay so what constraints allow us to do here is further tailor the solution to this problem here by eliminating values in the feasible set so what this does here is again let's restate our problem here so i'm gonna i'm gonna call this p4 now here but again it's it's the same cost function as case three right so i want to minimize uh j3 right this was our trade-off cost function here subject to elements in the feasible set here and i'm going to tack on some additional constraints here as something like what if i say i want to make some kind of requirement here that i need to get 30 minutes of exercise during my commute or more here so what i'm going to do is i'm going to make up a function let's call it f1 here right where f1's job here is that you put in one an element out of your out of your feasible set here and what this measures here is it measures the exercise time uh associated with action z right so the optimization problem becomes what we saw earlier but as well i'm going to specify a constraint like i need f1 of z to be greater than or equal to 30 minutes here so this is basically saying that on my commute i also want to make sure that i get 30 minutes of uh exercise what this does by specifying this additional constraint here is we see that it starts making some of these elements um in this original feasible set infeasible so for example if we look at all of these here let's let's just walk down these one at a time right how much exercise am i going to get out of uh each one of these so z1 here right z1 if i pass this through my f function it's going to look and say hey you spent zero minutes exercising because you're just walking in the uh driving in the car then you sit on your bum on the ferry doing nothing and then you drive some more so this actually does like nothing so this is now not going to be allowed any longer here so z1 is no longer a feasible solution same thing with z2 you're just sitting on the bus and sitting on the ferry so f1 this function here or this constraint that we cooked up is going to say no that's not allowed any longer okay z3 this is uh yeah actually this is still allowed because you're going to get a lot of exercise biking here on either end so this is allowed oh man z4 is definitely allowed right you're probably going to get more exercise than you want here and then the helicopter becomes infeasible because you don't get any exercise on this helicopter here right so suddenly this problem here now gets a little bit simpler because the feasible set shrunk so there's only two solutions here you can either bike or uh or or run and swim here right and usually for most values of r and q uh q and r right you're going to see that that z4 solution that's going to take that's going to take hours and hours and hours and hours so usually this this term here the the the the cost function that's measuring time is going to make that uh z4 solution really unattractive so in this case here what's probably going to happen here is we're going to end up with z3 is optimal right so this is really fascinating in the sense that depending on how you tailor your optimization problem right the solutions change here what is considered optimal changes here right so this is actually really interesting and now the question would be how can we apply this optimization framework to designing a control system for a linear dynamic system so um let me erase the board and we'll we'll jump into that next okay now that we've got that basic understanding of general optimization let's think about how we can set up the optimization problem for designing a linear quadratic regulator here so let's talk real quick about setting up the optimization problem so i always think it's easiest to think about a concrete scenario so you know what if we had something like like uh like a satellite here so i'm going to try to draw a satellite here i'm obviously not a very good artist i think those of you who have seen some of my other videos understand that um but i'll just make some little cartoon here for a satellite here and what we'd like to think about here is this satellite might have uh multiple states here right so the state vector for this satellite might be something like um the orientation right there might be states for the for the oiler angles there might be states for like the position um all these other things right we will just stack all these up in the state vector and at the same time this aircraft this satellite might have multiple different controls right it might have like a primary thruster here it might have smaller positioning thrusters or or momentum wheels or things like that here so we might want to also say that this thing has multiple controls here right so this would be like the like the main thruster um you know electric thrusters momentum wheels controls etc etc etc etc right all we're getting at is that this here is a dynamic system which has multiple states and multiple controls simultaneously let's go ahead and assume that this is a linear system governed by dynamics of x dot is equal to ax plus bu right our normal state space representation of my dynamic system basically saying that the state and the control they're not free to be anything they want right there's a relationship between how the states and the controls interact and they interact through this dynamic equation here right so within this context here what we want to do here is let's set up an optimization problem here so the optimization problem here is let's go ahead and consider a cost function let's just call this thing j like we did earlier here and we're going to propose this cost function here it's going to be the integral from time zero to infinity of x transpose times some matrix qx plus u transpose some matrix r u dt okay so here's our cost function here in this case we see that x obviously is your n by one state vector right u is our usual m by one control vector and here q is a n by n symmetric positive semi-definite and we'll talk about the uh definition of that here uh matrix here uh matrix here sometimes you'll see this written as q greater than zero or sometimes you might see q like this kind of scripty greater than zero here um again this is a notation for this matrix is positive uh uh yes positive semi-definite here and finally r is a m by m symmetric positive definite matrix so again sometimes written as q oh i'm sorry sorry come back to the sorry come back to the q here positive semi-definite means greater than or equal to sometimes it's written like this to contrast it with positive definite which means that it well we'll get to that in a second again you might see this written as something strictly greater than or again this this scripty greater than symbol here right okay um maybe now might be a good idea to to discuss what is the definition of positive definite and positive semi-definite let's come back to this and uh let's delete our picture here i think we understand the scenario that i like to keep in the back of your head so just keep in keep in your head that you know the state vector here let's let's think about this as being a state vector for the satellite and the control vector being as the control vector for this particular satellite here so the the concept here with uh positive semi-definite here so maybe let's write that down so positive semi-definite means that this matrix q if you took it and multiplied it by some vector x transpose and then multiplied it again by x this is a skate going to be a scalar number here if you if you do the dimensions here right this is n by one by an multiplied by an n by n multiplied by an n by one and excuse me when by the time you transpose this thing this becomes one by n right so you can see the whole thing ends up as a one by one scalar here right this scalar quantity is always greater than or equal to zero to zero for all possible x values here right so that's the definition of positive semi-definite is that this matrix is positive semi-definite if when you compute this weird quantity x transpose qx it is strict it is greater than or equal to zero for any value of x this vector right this this vector could have positive negative zeros whatever it doesn't matter by the time you run it through and compute this you get something that is strict is greater than or equal to zero all the time here right similarly the definition of positive definite right is that again you have this this uh this this matrix r here which we claim is positive definite the definition is that if you take some vector u maybe in this case transpose times u this is strictly greater than zero for all u right so again you can work out the dimensions and you see that this is a scalar number here so here's the definition of positive semi-definite and positive definite and what's interesting about that is that is exactly what are the terms of the integrand of the cost function here right so you see that the re the way that this cost function is set up here is that the integrand is always positive or uh or well actually yeah it's it's positive right uh for any x u combinations here right so you can never get a negative integrand the integrand is always positive so let's let's keep that in the back of our head as we uh continue discussing this okay all right so now that we understand positive definite and positive semi definite we have a we have a rough kind of um feeling for what this this cost function does right you can kind of see here that again the integrand is always positive and this q and this r those are sort of these weighting matrices or weighting values that we saw earlier when we were looking at our brief introduction to optimization right we earlier the q and the r were somehow to trade off between time versus money here right in this case we see that q and r trade off between states here non-zero states or non-zero controls we're going to take a closer look at that in a second but i wanted to plant that seed in your head right now let's start thinking about q and r as weights uh to to determine how much we value states versus how much we value controls here right okay so now that we have the the the cost function down the overall problem now in this scenario here is actually tell you maybe let's let's do it over here so we can keep talking about the the problem here so here's the cost function so the optimization problem let's write this again as our script dp symbol here right is all i want to do now here is i want to minimize this cost function j here right but um uh the controls i want to use here right there's m controls here so u is is a real valued uh m valued uh vector here right but we're not this is not unconstrained here there's a certain constraint right such that the single constraint that we want to consider here is that again x and u are not free to be anything you want here right it's x dot is equal to ax plus bu right so here is my optimization problem that i would like to solve here right and if we think about this long enough what it's basically saying here is that you want to take this cost function here and i want to find some control signal or some control law that is going to make this cost function minimal so so first of all let's stop let's think about that first ignore the constraint maybe for now right how would you make this minimal here so for example assume that your satellite had some non-zero initial condition like x of zero was not equal to zero in other words your satellite was cocked over at some weird orientation and some weird position not zero right something that you that you didn't want here right so this term is non-zero at time zero right now if the satellite stayed there right if you left it here like let's draw a quick picture maybe of just one of the states like x1 right if it's here's our our initial condition right so so maybe it is banked over at 45 degrees away from where you wanted to right if you left it here right and did nothing right what does this cost function look like here you can kind of see it already it's there's gonna be a square there's gonna be an x1 squared term here multiply by this q let's assume that q is like all ones or something like that effectively what the cost function there is measuring is it's sort of measuring the integral of this right of course it's going to be squared it might be skewed or weighted by that q value we're going to see that in a second but long story short here if this stayed here at a constant value this cost function is going to blow up to to infinity here right because it's it's the integral from time zero to infinity here right um so you can't have this here right this is obviously not the optimal thing to do here you can't leave the satellite cocked over at 45 degrees because that yields a cost function value of infinity here right so that's obviously not a good thing to do so instead what is better here right is we should try to bring the system back to the origin here right and now the cost here is finite right it's just sort of this area under the curve is is the the cost value so this is a better solution than letting it stay at 45 degrees so what that means is your satellite is cocked over 45 degrees what we're gonna do here is is maybe bring it back to zero that yields a better result here right however the the other flip side of that story here is how are you gonna bring the satellite back to zero here well you're probably gonna have to fire some thrusters right some positioning thrusters you're gonna have to expend some amount of control in order to do that here right so if we plot again i'm this is a real rough shooting from the hip description of this we're gonna look at it in a more formal fashion in a second here but let's just plot one of these controls like maybe u of t this is like one of the positioning thrusters or something like that right well what you're gonna probably have to do is is if you want to bring the satellite back to zero you're probably gonna have to spend some control authority to do that and then again you're probably gonna have to let this come back to zero here right so you're gonna actuate the controller and like you're gonna bring it back to zero then you're gonna stop actuating the controller right so the overall cost function you can kind of see it's sort of a combination of both how long are you away from zero in the states and how much non-zero control authority did you actually have to expend in order to get that here right so what you can see from this discussion here is what q and r do or q and r are these knobs that allow you to determine how much do you care about the state being zero versus how much do you care about the control not being zero here so let's talk about that real quick in general right what you end up seeing here is that um you know roughly speaking if q is is big compared to r so maybe we should write that down here right so if q is bigger let's put this in quotes here than r the reason it's in quotes here is because if you can you can see q and r they're two matrices they're square and they're symmetric but they're not even like the same size so how do you really say one is bigger than the other here well we're going to get to that in a little bit but you can kind of you can see the gist here right if q is bigger than r here what are you basically saying you're basically saying here that i really care that x is not equal to zero because if so this this term dominates here right so in this situation let me do this in another color here right this term dominates so really all you're saying here is the important thing in this case here is that if the states are not equal to zero you end up with a huge cost here right who cares about what the control is it's really the states that matter and drive this cost function here so in this scenario what do we end up with this is basically saying here the optimal thing to do here is do what you have to but make sure those states go back to zero as soon as possible here right so this is typically going to yield something like um you would have a fast regulation of x to zero right and in order to do that you're going to end up with u is large right so this is sort of the scenario where you're saying control is cheap i don't care what you do just make sure the state goes back to zero really really quickly here right let's flip that and and and and and compare with you know what happens if um r uh is bigger than q right so in that case we have the exact opposite scenario here right where now it's this term that dominates in the cost function right so the optimal thing to do here is make sure that you stays as small as possible here because if if you even flinches up for or flares up if you actuate one control just a teeny little bit this term in your cost function is going to blow up and you're going to end up with a huge cost here right so the intent under this situation the right thing to do here is use control very very very sparingly here right and who cares what the state ends up doing in this scenario if it stays if the satellite stays cocked over and just takes you know hundreds of seconds to come back to to the origin that's fine as long as you don't fire those thrusters because the propellant you're expending is is extremely valuable here right so the behavior that typically ends up happening here is you know if you have r bigger than q here you have very slow uh regulation of x to the origin here right and u is small right so this is really interesting here so you end up with this the the the first case here this is like an aggressive controller right and down here you have a conservative controller so we see that q and r are these knobs that are going to allow us to to to tweak the states and the controls or or or excuse me tweak the controller depending on how much we care about the states and controls let's look at a very concrete example the cost function and i think we'll be able to see that here so for example here right let's look at a a two state two control system right so what if your satellite here that we're looking at it only had two states like uh like we were looking uh at just the um sorry let me just write this down so i can i can talk at the same time here right there's only two states like maybe i only care about the pitch angle and the pitch rate or something like that of it and then i only have two controls here like i have a electric thruster and a single momentum wheel or something like that right okay and now let's let's pick values of this q and these r matrices here so in other words let's first start with something really really simple let's make the q matrix just diagonal and we'll have a q11 and a q1 uh sorry q22 let's make zeros over here similarly for the r matrix right r is just going to be an r1100 r22 right okay so these are these are these are symmetric and as long as all of these these entries are positive we'll see that these yield a positive semi-definite and a positive uh definite matrix here right so if this is the case let's compute this cost function here or more importantly maybe let's just compute the integrand of the cost function here um maybe let's see what can we do yeah yeah let's let's erase this i think we got the got the picture here okay so now what i want to do here is our is compute the integrand here right of x transpose q x plus u transpose ru right so run to mathematica and plug all this in here and what you'll end up seeing here is that this integrand looks like it's it's pretty simple it's a q11x1 squared plus a q22x2 squared maybe let's put the squared like this sorry um plus an r11u1 squared plus an r22u2 squared here right so here i think you can very clearly explicitly see what do these q11q2 and 22 and r11 and r22 entries uh physically mean right if you look at this right we see that q11q1 is sort of the uh the weight or the the penalty effectively on a non-zero x1 right because it shows up right here right so if x1 is not zero right um if it's positive or negative by the time you square it this term is going to be positive here right so as long as the magnitude is not zero this square term is something uh positive and q11 is now this this multiplicative factor here showing you how much does it matter that x1 is not zero here right it will either exacerbate that in terms of the cost or it would attenuate it depending on on this value here right and similarly let's just go through all of these just to be very very explicit so q22 right this is the weight or the penalty on a non-zero x2 right so we very clearly see that the q matrix by choosing the entries appropriately you can now say which state do i really care about being non-zero right i can either say that i really care about x1 being non-zero or i care about x2 being not zero similarly the r11 term right you see it right here right this is the weight or the penalty on a non-zero u1 of t right and similarly r22 is just the weight or the penalty on a non-zero u2 of t right so r11 is basically it's it's actually telling you pretty much exactly how expensive is it to fire or to to utilize control number one right yeah this was like maybe your your electric thruster maybe it doesn't cost you anything at all right because that's the whole deal with electric thrusters is that electric thrusters are are are cheap um they are long lasting they're reliable here so so if you want to model that fact here in your control design you can maybe pick a pick a small r11 right because if r11 is small it's basically saying i don't care what you're doing with this controller this controller is super duper cheap because even though if this control is is is large by the time i multiply by a small r11 this doesn't contribute to the overall cost function much right and in the same fashion you see that r22 is is the is the cost of actuating controller number two here right so that's pretty awesome this i think allows us to set up the entire optimization problem here right now the thing that's interesting here is now if we start coming back and thinking about um the the constraint here right this is the interesting part we i think we understand the cost function at this point right we understand how to tailor or to to tune this cost function using this q and this r matrix here right but the thing that we have to do now is we have to solve this overall problem now here subject to the linear dynamics of this system here right so that's where it gets interesting so why don't i pause the video i'll erase the board and now let's talk about after we set up the optimization problem how can we solve this optimization problem okay so now what we'd like to do here is look at solving the optimization problem so graphically what this means here is again here we've got our linear plant right which is governed by x dot is equal to ax plus bu right and now what i want to do is i want to find a control law u here which is going to make this whole system here um optimal here right so i'm going to choose a u here so that the x uh by the time i look at the combination of u and x in that cost function it stays small according to the cost function that we set up here right so um it turns out that the solution to our problem here so i'll just write this down here so the solution to our optimization problem here right which was we said earlier minimize this cost function of integral zero to infinity of x transpose qx plus u transpose r u dt uh subject to these dynamics here right this was the optimization problem here well some really smart people figured it out it's beyond the scope of this lecture today to derive it here but i think we can use their results here the solution here is that you better choose a control here that looks like this minus k x of t where this gain k here is given by r inverse b transpose s here and s is the solution to what's called the algebraic ricotti equation of this so a transpose s plus s a minus s b r inverse or r inverse b transpose s plus q equals zero here right okay great um and here yeah s is the solution algebraic ricotti equation and also it is it's size n by n and square and symmetric okay so this is the solution here right so tell you what if we first if we first ignore everything below here just look at look at the solution here right the solution is minus kx we've seen that before this is just a full state feedback controller right so in other words the architecture that goes along with this is that this thing just looks like minus k right so the optimal thing to do the absolute best thing you can do to solve this problem is to use a full state feedback controller here now what gets interesting here is that the way you get this k here you have to go through a couple of steps here right if you look at this so step one here is we got to solve the algebraic ricotti equation for s here so let's talk about this the so the procedure for lqr right so step one here right we are going to be given our a and b matrix right those come from the plan right those are known right okay they're known and immutable you can't really change them the next thing we have to do here is you you are going to pick the q and the r right because you as a control designer you get to choose how much do i care about states versus how much do i care about control um actuation here right so you choose the the cost function effectively here right so um let's say step two is choose q and r right you're basically designing the cost function now according to this step three here is once i have q and r i need to solve the algebraic ricotti equation for s here right so solve i'm just going to abbreviate this as a r e here right so this down here is the algebraic ricotti equation here right okay so you solve the algebraic ricotti equation for s all right what you're going to end up seeing here is that once you solve for s there's actually going to be multiple solutions to s we'll take a look at this in a second here but if you follow a couple of steps you'll be able to find out which is the one you want here once you know that here um well actually no i take that back let's just say solve this thing for s you're going to get multiple solutions therefore you can then compute k you're going to get multiple solutions for k here right so you can go ahead and compute your optimal gain k using our expression over there right r inverse b transfer transfer transpose s here right and again since there are multiple solutions of s you'll get multiple solutions for k here the last thing that you have to do then is typically there's only going to be one solution here that yields a stable situation here so um if you remember earlier with this control of a full state feedback here right this thing is basically going to behave at the end of the day like x dot is equal to a minus bk x right and this here is like maybe you're you're a closed loop um uh a matrix here right again if if this is if this is brand new to you here maybe take a quick refresher here to our video where we discuss full state feedback controllers but we show that under this control law the dynamics reduced to this here right now what you can do here is you've got multiple k's here right so that means you have multiple a closed loops you can evaluate each of these eight closed loops and find its eigenvalues and usually there's only one of them that are going to have all poles in the uh or eigenvalues in the left half plane here so let me let's say step five here is choose um the case solution that yields a stable system yeah so that's the workflow that we need to go through so uh again i love concrete examples let's look at a concrete example right now to try to implement all this so the example i'd like to use here is a really simplified scenario it's a um a mass and a damper that's it okay so let's look at an example here of a mass uh slash damper system so you got some smooth surface like i don't know like some ice or something like that you got a mass sitting on this thing and all we're going to be doing here is let's consider the position and the velocity here like such positive position and positive velocity is to the right here and let's say that you have um a single control of this you're able to exert some kind of force on this block so you maybe have one rocket engine or one thruster that pushes this block along this ice here and here there's some viscous uh damping between the block and the the surface here all right so i think you can get without too much of a stretch of an imagination here that newton's second law equations of motions are going to basically give you mass times of a law uh mass times acceleration right is equal to the sum of all of the forces here right so you got the force here minus the retarding force of the um ice here right p times c or sorry c times p dot here right okay so without too much um sorry i guess we should say viscous damping with coefficient c right that's what this c is here it's just how much friction you got okay so um without too much of a of of a stretch i think you can turn this thing into a state space representation here by choosing a state vector x of t as let's make it the position and the velocity of the block that's our state vector and the control vector in this case is actually really really simple there's only one control it's this force right okay if you do this here you can basically get yourself a uh a state space representation of x dot of t is equal to uh zero one zero minus c over m x plus zero one over m u great that's our state space representation here's our a matrix here's our b matrix here let's go ahead and choose some numeric values so we can actually try to compute some of these here so let's go ahead and how about consider a mass of one and a coefficient of damp uh damping here of 0.2 so numerically we end up with an a matrix that looks like 0 1 0 minus 1 5th here and a b matrix that looks like uh 0 1 okay something pretty simple now that is uh that's step one here right we got a and b okay step two here is let's choose q and r okay so i don't know let's pick like a q of just uh something really simple how about an identity matrix here later on we're going to look at some examples where they're not identities but uh okay q is identity matrix r i don't know let's choose something like 0.01 something like that okay great so we got step one and step two now done now we got to go ahead and solve ourselves the algebraic ricotti equation here so um that's a little bit of a pain in the neck here right because if you remember that's this long matrix equation we got to set it equal to zero and solve for s here so i don't want to do that on the board let's do actually steps three through five here let's just run over to mathematica and have it do this for us so we can um see how can we compute this gain k all right so i'm going to pause the video and i'll meet you over on mathematica all right so here we are in mathematica so why don't we go ahead and uh make a couple of notes here so what we would like to do now is demonstrate solving the ar the algebraic ricotti equation and computing uh and lqr controller k right that's our game plan here so we said uh let's first do step one right step one was define the a and b matrix right so i think what we said was a was just a very simple uh what was it zero one zero minus one fifth i think and then b was uh pretty simple zero and one okay there we go shift enter that guy and here's step one uh what was step two step two was go ahead and choose q and r right so i think in our situation we said that the q was just going to be a very simple identity matrix here and the r matrix is actually going to be a one by one um of one over 100 here so let me just define it here as a list of lists here it's just that mathematics has a little bit of trouble using uh the parentheses notation to make a one by one matrix here so i'm going to just explicitly make a one by one matrix here um okay shift enter both those um okay now we're on to step three we said which was solve the are right so just to refresh your memory here right um the the algebraic ricotti equation is given by what do we say was i think it was a transpose s plus sa minus s b whoops r inverse b transpose s plus q equals zero right so we want to solve this for s here right okay so let's just go ahead and compute the left hand side of that expression first so the left hand side here is what it's just transpose a times s plus s times a minus s times b times inverse of r uh times transpose of b times s plus q right so that's the left hand side here and maybe let's just simplify maybe let's put the simplify up here whoops did i spell that wrong simplify here right and let's just look at this thing in matrix form just to get an idea of this so uh wait a second oh i'm sorry here we should have defined s here right so um we we uh should know here that s is what it's a symmetric matrix here so there's gonna be an s11 and s12 and this is also going to be an s12 because it's symmetric and then we'll have an s22 so this is what s looks like here so maybe let's make a quick note of that here in the mathematical notebook here so uh define s as a symmetric matrix of size n by n right okay now this second step here is go ahead and solve the are okay so let's see uh shift enter this value of s here shift enter the right hand side here we go so here's what it looks like right so we see now we basically have um again this is actually a symmetric equation here right or symmetric matrix if you look this off diagonal entry is the exact same as this off diagonal entry here so really we have three equations and three unknowns right i need to find s11 s12 and s22 here right so what i need to really do now is i need to say solve basically the uh a bunch of equations right it's the left hand side the 11 element right this has got to equal zero actually all of these entries have to equal zero so this has equal zero the left hand side of one two has got to also equal zero and the left hand side uh 22 has got to equal zero right so those are the this equation has got equal zero this has to equal zero and this has to equal zero here right those all solve them simultaneously for what s11 s12 and s22 right that's what i'm looking for here right so if i shift enter this you see that we actually have four solutions like we said earlier right that was to be expected we said that when you solve this for a lot of times they're going to be multiple solutions for s so let's just go ahead and um extract those here so maybe let's assign this to a temporary variable call it temp and um let's see here uh s um let me think about this how do we want to do this here uh you know actually we already defined s up here i i know what we'll do let's just do it like this here so um let's call this thing how about solution one gets a temp one so this is the first set of solutions here all right um let's do this for actually all four of them here right so let's get all four solutions here so let me see change this to a four three two and then solution two three four okay so here's our four solutions and what i want to do now here is make the four matrices here so s1 is going to get s but i want to make all of these appropriate substitutions so i'll just say slash dot with solution one all right so here's s1 and great let's again do this for all four four three whoops four three two and then this should be two three four here and tell you what let's just go ahead and suppress the output for all these and then we will just look at these things as numerical values in matrix form just we can get a rough idea of what they look like oh sorry that should be s1 s2 s3 and s4 so we shift into those so we see here are the four solutions here to the algebraic ricotti equation so we need to now identify which one of these are the correct ones so let's move on to step four here step four which was compute k right and if you remember here we said that the way we compute k here was k um sorry excuse me uh k is going to be what it's r inverse b transpose times s here so we end up with four solutions here so let's do this here so k1 is inverse of r times transpose b times s1 here so that's the first possible solution and makes me let's do this let's make let's get all four of these guys two three four and then four three two okay great and then let's look at their actual values here for fun k1 slash slash um n slash matrix form and again let's look at all four of the different controllers that we've now computed here here we go so you can see they're they're they're very different here they have different signs they have different numeric values here so theoretically or at least mathematically all of these solve the problem but we know from an engineering standpoint here that they can't all work only the ones that have stable uh closed loop eigenvalues um actually will give us a system which goes back to the origin and therefore has minimum um minimum cost in our cost function so now step five here is we need to find solution that yields stable system so in other words what i want to do here is i want to look at the eigenvalues of a minus b k right and uh let's call this here here's my first set of eigenvalues is is lambda one and again let's just go ahead and do this for all of them all right so here's lambda two three four and this should be four three uh two okay and then let's just look at them all here so uh lambda one uh slash slash n and let's just do this for all of them we'll look at numerical values for all of these guys right so two three four all right shift enter that here we are so if you look at this okay obviously this one's out we got a positive poll we got a positive poll got two positive polls aha here we are it's actually the fourth solution here right the fourth solution gets you a controller which will yield negative um polls so we see that the solution here that we're looking for it's really k4 here so here's my controller here that should work a positive 10 and a positive 10.75 okay great so we just did this in mathematica and we saw that basically this a b q and r matrix when you run it through the procedure yields a gain matrix k of what do we say this was this was 10 and 10.76 here right so perfect we designed our full state feedback controller using this technique but as you saw it's a little bit cumbersome right you got to go through all of these five um operations here luckily for us matlab has taken all of these operations here and packaged up it up into one function called lqr so if you pass lqr these necessary inputs right in other words you give it the a b the q and the r matrix this lqr function will basically go through this operation and return to you the gain matrix k it also i believe will give you the solution s here that we were solving for as well as the closed loop eigenvalues of your system here so in other words in matlab all you got to do here is call lqr pass it a b q and r and it will return to you k s and e here so again here k is going to be your um full state feedback gain matrix uh s is going to be the solution to the algebraic ricotti equation and e are going to be the eigenvalues of the closed loop system right a minus b k right great so what i'd like to do now here is lqr is as we see it's this it's this powerful tool here that's going to allow us to change the behavior of the controller k here right by specifying the q and the r here so i'd like to investigate now three scenarios so maybe let's make ourselves a little table here so scenario maybe with a description um and we'll see what is the resulting k here okay so let's make ourselves a little table here i'd like to investigate three scenarios one two three three so in the first scenario what i'd like to look at here is um let's go ahead and investigate the situation where control is cheap and a non-zero state is expensive right um and then in scenario two maybe let's get ourselves a little bit more room here scenario two let's look at sort of the opposite scenario here where control is expensive and a non-zero well maybe i shouldn't i mean let's write this out where a control is expensive and a non-zero state is cheap okay and then finally the third scenario here is let's let's let's mix this up a little bit let's say how about only a non-zero velocity is expensive okay so in this first scenario right you're saying uh that maybe um it i don't i really don't care how much control authority you have to exert i just really want to make sure that the state goes to zero quickly the second scenario is the exact opposite whereas maybe back to our satellite example where you're flying to mars here and and and fuel is extremely expensive but you have all the time in the world to kind of make sure that the the satellite returns to its appropriate orientation here and fine the third scenario is kind of interesting it's saying i only care about the velocity i want to make sure that the velocity doesn't um stay high i really don't care about the position here so what i'd like to do now is and actually maybe instead of k here first let's talk about translating this into an appropriate q and r matrix okay so you can kind of see in this first case where we're saying q uh control is cheap and non-zero state is expensive uh here that's almost like the scenario we just looked at here so with a q matrix of an identity matrix one one zero zero and an r matrix of zero point zero one right again all we care about is sort of the relative sizes so you can see here that r is much much smaller sort of relative to q so that should hopefully translate into a scenario where uh control is is cheap you have an aggressive controller here and in fact we saw let's write down exactly what the gain matrix k was coming out of this scenario here so we actually just solved this here so the k in this case was 10 and 10.76 right okay now let's look at the second scenario here where we would like to flip this so all we got to do now in this scenario is effectively sort of change the sizing the relative sizing of q and r so maybe let's keep q the same here so let's let's use another identity matrix but let's just jack up r we got to make r much bigger than uh than q effectively so maybe i don't let's let's go with a thousand here right and now the question is what is going to be k so we're going to run over to matlab and use lqr to do that before we go to matlab just so we don't have to break tempo so much let's fill in a q and an r for this third scenario here so in the case where we only want to penalize velocity here right we can kind of see that one way to do that here is by choosing your q matrix appropriately so for this q matrix what we would want to do here is if i only care about the the velocity being expensive i just got to make sure that the q entry that is associated with the second state is big relative to the q entry associated with the first state so maybe let's do something like this 0.001 and then uh 10. so as you can see here they're they are clearly um mismatched so the second state will be penalized orders and orders of magnitude more than the first state here right and then finally the r matrix i don't know let's just choose one here right because in this scenario all we want to show is the mismatch between the two states all right so now that we've set this up we've got all the different cues and the r's let's run over to matlab and have it basically we're just going to call lqr three times on each of these scenarios and see what the resulting matrix k is and then why don't we simulate that using uh our full state feedback controller right because we said that once you get this this this gain matrix k the optimal thing to do here is to have your plant right your x dot is equal to ax plus bu right and we're just going to wrap this back through a minus k and let's use some initial conditions i don't know maybe um in in here let's use initial conditions of uh what did i end up using here uh let me just make sure oh here's something like maybe let's make it pi and uh negative 2 here right so this is the position it's pi units away it's moving at negative 2 meters per second velocity here and let's see how fast this non-zero initial condition will be regulated to zero in these three different scenarios all right so here we are in matlab and i've actually already typed up this script just so you don't have to watch me inputting it in real time here um and we see that this is really simple all we're doing here is defining our a and b matrix as we had on the board and now the q and the r matrix we saw that the only difference between the three scenarios of where we either want cheap control expensive control or the scenario where we ignore the position and only care about velocity the only difference is the q and the r matrix right those were our tuning knobs to tailor the optimization problem for our specific desired application so i've just got a switch statement here where depending on what scenario we're investigating i will just load in and define the q and the r matrix that we discussed on the board once i've got the a b q and r matrix we saw that the entire l qr process is now packaged up into one nice function call in matlab namely l qr so i give it the a b q and r matrix and it will give me my full state feedback gain k the solution of the algebraic ricotti equation s as well as the eigenvalues of the closed loop system here so i'm just going to print that out to the screen so we can see what the the controller looks like and once i've got that defined we can think about running a simulation to examine how does the system respond under this controller so i've got um just a couple of parameters here like i'm going to change the final simulation time and then we're going to define the initial condition of this of the state space model as we discussed on the board here so let's build a simulink model now that will simulate this system i've got myself a blank uh simulink model here saved and ready to go so it's really simple the only thing i need to do here is first let's grab ourselves a state space block here and open it up and change the parameters to be a b and now the c matrix right this needs to be a uh identity matrix of size two and the zero uh the d matrix is what it's basically zeros of size two by one right all right so this here now is a state space system that we defined here where the output is just the state okay and we'll put in the initial conditions like we discussed earlier hit okay and now the controller in this case it's just a full state feedback controller so it's just really a simple gain here of magnitude minus k and i just need to change the multiplication from being element wise to being a matrix multiply and we basically have now maybe a better name for this instead of just gain here is this is our full state feedback controller computed via lqr right so this is my lqr controller effectively right so let's just hook everything up oh come on connect there we go connect this up together and we basically have our simulation ready to go um oh wait we should maybe change this for a simulation time let's make a t final like we discussed and also what we probably want to do is send this results back to matlab so we can analyze it so let's grab a two workspace block and i will change this to be sim x here and let's change the save format i just like structure with time better than time series and let's go ahead and save both the um the uh output state here and then you know we should also do is let's copy and paste this and we'll get another one which is going to be sim u and i think we should be good there and let's hook this up uh whoops sorry branch off and hold there we go save this guy we should be able to now close this model here it should be ready to go and now what i'm going to do here is let's just go ahead and simulate the system now all right so now let's just go ahead and run that simulation here so i'm going to call sim intro to lqr and once that simulation has finished let's go ahead and extract the data here right so time is going to be uh sim x dot time and then x1 is going to be sim x dot signals dot values all rows column 1 here and let's do the same thing for x2 and u1 at the same time here so extract the second state and the first control here and now we've got those extracted we can go ahead and plot these right so let's go figure subplot 3 1 1 and let's plot time and x1 and let's make this a little bit thicker so we can see it a little bit more easily and we'll maybe add a legend as well just so we can discern which one is which and let's do the same thing for x2 and u1 while we're at it here and maybe what we should also do here is um let's add a title to this first subplot here so we can denote what the scenario is okay scenario there we go okay great so this looks pretty reasonable so let's go ahead and run this script here and first off we see yep this is the gain matrix k that we computed it's the same thing um that we computed manually let's go ahead and look at this solution of the algebraic ricotti equation and again this should be the exact same values that we got with our manual calculation in mathematica so this all looks pretty reasonable now let's look at the results here so remember this is the scenario here of cheap control so we see that what cheap control means here is that the system or the controller decides that i better make sure that the states go to zero fairly quickly so you see that in just five seconds both the position and the velocity of the system have gone to zero here and we spent around i don't know what is that minus nine minus 10 units of force in order to do that here so let's compare this now with maybe scenario two and maybe what we should do is let's keep this plot up so we can compare so maybe let's come to the top of our script here and temporarily comment out this close all command and let's just change our scenario now to scenario two here so scenario two is now let's tell the control system or tell the lqr procedure that control is really expensive now so if i run this again you see the k matrix now is smaller so we have a less aggressive controller here so if we look at this now here's scenario number two and if we compare that with scenario one maybe let's put scenario one on the left here and let's put scenario two on the right you can see the differences in control right so now the control system or the lqr procedure realizes that you know what um control is very expensive this is now like rocket fuel that is very uh costly so what i'm going to do is the controller is going to allow the state to be non-zero for a longer amount of time right so you see over here it takes almost 30 seconds for the position and the velocity of the block to come regulated back to zero but the benefit is if we compare the control that was expended here you can see the magnitude is much less right instead of having minus 10 units of force we're now we're now you know two orders of magnitude less here right and great now finally let's look at our third scenario here so i'll come back to the script here and change it to scenario three where we're going to now ignore position and if i rerun this thing again again you see that the controller changes and more importantly the behavior of that controller changes so now take a look at this this is pretty awesome the control system realizes that you know what i better make sure that the velocity goes to zero so it does that quickly but it doesn't give a hoot what the position is doing look at this the position is sitting here and still after 30 seconds it's nowhere near back to the origin but who cares right the controller realizes that the position does not factor into the cost function much at all so it's going to choose a control strategy that only pays attention to what matters namely the velocity and the control signal here so this is pretty awesome if you think about it we've now got a tool that's going to allow us to translate between physical requirements like which states and which controls do you care about and then synthesize a controller that will respond and respect those uh engineering requirements okay so that was pretty interesting exciting results so i think we can come here we can fill out our table here and finish this section here so again we saw the situation where control was cheap and a non-zero state was expensive we had this type of a controller come out of the lqr process but now when we change that r matrix we saw that the control was much less aggressive i think what 0.03 and 0.12 so yeah you can just look by the magnitude of these two you see that the second controller is is much more conservative right and then finally the third situation where we only cared about the velocity you know it's a little harder to interpret this controller here but i think we ended up with something like 0.03 and 2.98 here but again we might not be able to interpret directly what these numbers mean here but we saw the behavior was exactly what we wanted here right in the sense that the uh controller drives the velocity to zero fairly quickly but doesn't care so much about the position here right okay well this is uh we're really cooking with gas now so give me a chance to erase the board here and we'll come back and apply lqr in another way okay so now that we've got a rough handle on what lqr is and what it can do let's talk about how about using lqr to address some of these practical full state feedback control issues so if you remember this is actually what we discussed in uh our previous lecture here so again i i would encourage you to check out that video if you haven't done so here because i think it will set the stage for this discussion now and you'll have a better understanding of the context but um if you don't want to maybe just a real quick recap what we looked at last time where we were talking about full state feedback issues is we looked at a plant model here which was a uh a dc motor right and the idea with this plant model here was there were two control inputs there was a voltage on the armature of the motor as well as an external disturbance torque you could apply to the motor and what this thing did here is the full state of this system here was the position of the motor the velocity of the motor and the current of the motor here so um using this state vector x of just uh theta omega and i and a control vector u of just va and this tl which was the armature voltage and the torque here we saw that numerically you got an a matrix um that we were playing with it was a three by three here it looked like zero one zero zero minus zero point two nine and seventy one point nine three and then finally zero minus sixty three point two four and minus ten twenty point three five here and then your b matrix we actually we're just looking at the b1 here right the first column so what we did here in the example last time is forget about this external torque here pretend that this system only has one input which is the armature voltage therefore the b matrix only has one column and this is going to be zero zero uh 641.81 something like that right just to give you some numerical values that we can play with now right so this here is our model of this motor here right and what we said last time here right was that you could design a full state feedback controller for this system but we saw that we ended up with some issues right some of the issues we we encountered when designing a full state feedback control system was control saturation right and two was the inability to measure all states right you couldn't measure the full state uh every we didn't have a sensor for every single one of these i think we only had a sensor for for one of them right so last time we looked at some uh i don't know if i want to call them hacks here but they were they were ways that you could try to address these right we said that full state feedback controllers were incredibly powerful right but but as spider-man's uncle said right with great power comes great responsibility we have to deal with both of these issues here if we're trying to actually implement them right well lqr is going to give us a technique to directly address this so let's talk about how can i use lqr to address these two issues here right lqr is going to give us a perfect way to do this so let's look at first how about control saturation right i think the example we showed last time was that if you were if you naively went about trying to place the poles of your closed loop system right you could get controls which was in this case vas in terms of volts right this could spike and this could go down i think the situation we saw was this went down to like something like minus 90 volts right so you had this huge power requirement so so actually in this case it's it's with great power comes great electricity bills here right so what you know if ben parker was like peter uh you can't be doing this right our electricity is going through the roof peter parker should have just said oh geez sorry uncle ben next time why don't i i'll use an lqr controller to design my full state feedback system so it doesn't have this this problem right and we saw the way that that manifests itself here is that peter could just go ahead and vary the r matrix to make this thing um as aggressive or as conservative as he wants here right so what we can do here is the solution is just use the r matrix to uh tune quote unquote the aggressiveness of the controller so just as an example here i'm going to run some numbers here let's use a q matrix of just an identity matrix right and then the r matrix is just going to be a single value right let's call it an r11 here right and we see that all this is doing now is r11 or this r matrix we can tune how much do we care about this va measurement here right so let's just make a quick table to show what this ends up being here so let me r11 let's just make a couple of tables and then we'll look at the resulting uh k1 k2 and k3 values that are going to come out of lqr so what we're going to do right now here is you could run over to matlab and you can say matlab hey lqr of a b q and r right and this is going to spit out the gain matrix k here right so i would just like to do a quick little study to see the how how sensitive is k2 r right if i change r how does this change the result in controller here right so i just ran a couple of numbers here so if you just turn 0.01 here you would get a k of 10 about also again around 10 and about 8.7 something like that so here's one controller and this might yield something like this right where you have huge power requirements well all you need to start doing then right is peter should have just sat here and cranked this number up and see what happens so if you crank up r you're basically saying control gets more expensive so what happens to the controller well it goes down right it starts getting smaller here right so you can see it's getting less aggressive and if and if and if and if uncle ben is still yelling at him for eat it for for costing him too much electricity he just cranks this thing up again right here so one you would get something like 1.0 0.93 and 0.34 right and then again you just for giggles let's crank it up even more so 0.32 0.25 and 0.05 here right so you basically see we get this trend here as r increases right the aggressiveness of the controller decreases right decreases so we see r is this knob it's literally the knob of depending you can tell the system how much do you care about each different control um when trying to drive the state to zero great okay so that's one issue what about what about the second issue this inability to measure the full state of the system um maybe let's leave this up here let's just tell you what let's just erase this and we'll go back do the same thing here uh we'll start from this location and you can see that it's it's it's a very similar story here right so problem number two here right was the uh inability to measure full state so um unfortunately this is still a little bit nebulous even with lqr right so if you remember previously in our earlier discussion right what we attempted to do was we attempted to identify which eigenvalues were associated with which states here so let me write this down here maybe so previously right this is without lqr knowledge what we tried to do was tried to identify or id which eigenvalues uh were associated with which states and again let's put this word associated in quotes because we saw that you needed some eigenvector analysis to actually do this and for most systems the eigenvalues are coupled to multiple states so they're kind of tangled and you usually can't directly do this here right but we tried to do this here because what we wanted to do is once we identified which eigenvalues were roughly associated with which states we only tried to move those poles or those eigenvalues in the hopes that it would yield the control system which had which didn't require you to measure all of the states only to to uh measure the states or only to use the states which you could directly measure here right well luckily for us we can lqr is going to give us a tool to skip this here right because if you think about this now right with lqr right i don't care about the relationship between eigenvalues and states because i have a tool or a tuning button that will directly allow me to penalize states directly here so with lqr what i can do is i can use q right that q matrix to directly um specify which states are important or measurable in the sense that i have a sensor to measure certain states here right so now i don't need to do this eigenvalue to state mapping i can just use q to directly ask what states do i care about here right unfortunately this is still not a perfect solution here because there still is this coupling right so again let's go to an example here so for example what we might want to try now is this q matrix right let's just make this like a q 1 1 and then zeros everywhere else right because what we're saying here in this case is i only care about the first state which was the position of the motor which happened to be the only thing we directly measure here right so the hope here is that by just changing this value here or by only penalizing the state uh theta we're going to cross our fingers and hope that the lqr procedure is going to kick out a controller which doesn't really require me to use omega or i much here right again this is a little bit shooting from the hip here because it's not entirely true we're going to see that this this is a case here so tell you what let's use this q and an r of just one here right so in this case we can do our same thing let's make ourselves a little table here of what q 1 1 is right and then let's look at the resulting uh k1 k2 k3 again by using our little our lqr technique right so shoot i didn't leave a lot of space here but you guys get what i'm saying right we're basically going to go here and say lqr of a b q and r and this is going to give me the k matrix here right but now this q is i'm only penalizing this first state here right so again you start here as a small q 0.01 or something like that you'd end up with this of 0.1 0.02 0.0014 here right and actually another quantity that might be useful in this discussion right remember what we're trying to do here is we're we're crossing our fingers and hoping that this k matrix you know we're trying to implement sorry we're trying to implement the control law the control which was va right is minus kx right what we would really like is if you write this thing out this is a minus k1 times theta minus k2 times omega minus k3 times i we're hoping that these k1 k2 and k3 are small with respect to k1 so that we can safely neglect them and we end up with a control law that is realizable in this case in the sense that it is gains multiplied by states that you actually measure here right so in other words we're hoping we can neglect these two so what might be useful here is we actually should maybe let's look at the ratio of of k2 over k1 and k3 over k1 so let's write that down so k2 over k1 uh 1 in this column and k2 over i'm sorry k3 this should be k3 over k1 right because this gives us an idea of how big are k2 and k3 relative to k1 here right so in this case i think you end up with this is about being 0.19 or 19 here and 0.014 here okay so in this case i don't know if i would say this is is is super great here because you see that the second gain is almost 20 percent of the first one yeah this third gain is nice i think you could safely neglect it because it's only 1.4 percent of it but um yeah tell you what let's start cranking up q1 here to say this is that that that that theta is the is really much more important so again we can keep working on this table here right so um move this up 0.1 here right so in this case you would get a k1 of 0.32 0.05 and 0.0037 here so in terms of fractions here this would be 0.17 and 0. 0.012 so it's going in the right direction right the importance is going down now it's only 17 percent great let's keep cranking this up let's go 1.0 here right so if you go one you would get a k1 of actually one as well and then 0.13 and then 0.0092 so in terms of fractions here this is now only 13 percent here and now we're down to less than one percent so finally let's do one more just for giggles here right so crank k11 up to 10 here right so now you get something like 3.16 0.29 and 0.02 here all right okay great and now we're down to 0.09 and 0.006 so here we are so you can kind of see that now in this regime down here i think this is getting safer to neglect k2 and k3 because it's only 10 percent and less than one percent of the overall controller here so again we see that lqr gives us another tool to try to address this inability to measure full state but it's still not a uh a panacea here right we still get um some issues and there will be some error when you make this this this this this neglect here right when we neglect these other two terms you're basically knocking off 10 and 1 percent of of the controller kind of here right again you got to worry about what are they being multiplied by it's being multiplied by omega and i so this is again a little bit uh off the cuff here right okay so um this is pretty awesome i think i think we had a good discussion and maybe this is a good spot to leave it let's just quickly summarize what we ended up discussing today with our introduction to lqr so in terms of a summary all right what did we say we said um all right lqr we see it's a great way to generate generate a controller that's optimal here in the sense that it solves this optimization problem right we said that the whole deal with where lqr came from is we wanted to minimize this cost function j which was integral from zero to infinity of x transpose qx plus u transpose ru dt all right let me get rid of some of this other stuff so we can have a nice summary slide okay okay so we were minimizing this and we were picking um at first we were thinking about any control law or right your control can be anything but then we had to impose a constraint here that really your x's and your user not completely free they're coupled together and the way they're coupled together is just our our linear state transition equation right so you're basically saying you as a control engineer give me the q and the r here and this optimization problem needs to solve a controller that will respect the dynamics of the system here right and it turns out here that the optimal control for this situation that solves this problem here right so we'll write the optimal solution is this controller u of minus kx where k right is um r inverse b transpose s here and s is the solution to the algebraic ricotti equation that we talked about earlier right so we saw that the actual solution here is nothing more than a full state feedback controller where k is computed in a very particular fashion here right so what's so awesome about this uh lqr technique here is that as a control engineer right right lqr gives us two knobs to directly penalize states or controls right so as a control engineer you choose the q and the r matrix here right and then you run the lqr process here and it will will synthesize a controller for you which respects these uh this cost function here and tries to find something that is the best here right so q and r are much more helpful knobs to twist because they have direct physical interpretation right q will literally tell you which states you care about in your system and r is going to tell you which controls are important or cheap or expensive or or whatnot here right so um i think that's a good spot to leave it here uh i hope you enjoyed this video on lqr here some of these things set the stage for some of our uh future discussions namely linear state estimation um is one of the topics we're going to cover in the future so with that being said um if you like the video please subscribe to the channel because like i said we'll have these future discussions on other controls topics and other engineering uh issues in the future so i hope to catch you at one of these future videos bye
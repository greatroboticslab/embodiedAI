tFVAaUcOm4I
https://www.youtube.com/watch?v=tFVAaUcOm4I
Unknown Category
A PID controller is a control structure that's used in many closed-loop systems. It was developed by Nicholas Minorski in 1922. PID stands for Proportional, Integral, and Derivative. It's a way to compare the input and output of a system to generate a signal to control some process or machinery. Don't worry about understanding this diagram quite yet. We'll work our way up to it throughout this video. But know that PID controllers are one of the most popular control theory mechanisms. If you're not familiar with it, control theory is a field of engineering and mathematics dealing with the control of dynamic systems. Often, these systems are mechanical and can be described entirely by math equations. I'll give you an overview of how PID controllers work and some examples of them in everyday use. Let's start with our system or subsystem that we want to control. This can be any process, piece of machinery, actuator, or what have you. In control theory, you'll often see this referred to as a plant, which is a term that comes from the Industrial Revolution in the late 1700s to mean something that has been planted in the ground for industrial purposes, such as machinery or tools. The process receives some sort of input signal, and the output can be ideally measured. While controllers are extremely popular in industrial settings, let's start with a more common example. Most cars have an accelerator, also known as a gas pedal. When you press the gas pedal, the car responds by allowing more air and fuel into the engine. As a result, your engine produces more power, which increases your speed. Now, most accelerators are not binary. You can control the amount you depress the pedal with your foot. Press a little, and some air and fuel go into the engine, and your car starts to accelerate slowly up to some speed. Mash the pedal to the floor, and you'll start dumping a lot of air and fuel into the engine. You'll accelerate quickly, and at some point, you will reach the maximum speed for your car. Our car is an example of a process or plant. We can give it an input signal, the accelerator position in this case. The car will respond with motion, and we can read the speed of the car on our speedometer. That's the output. Let's say we want to maintain some constant speed. As it turns out, our brains are very good controllers. We can often read the speedometer and adjust the gas pedal as necessary to maintain that speed. Keep in mind that on flat roads, you can just hold the pedal in one spot to maintain speed, but you will need to adjust the accelerator position when going up and down hills and around turns. This is a closed-loop system. We read the speedometer or judge speed by looking out the windshield and adjusting the input signal, the accelerometer in this case, as needed. However, let's say that you, the engineer, have been tasked with designing a controller that will automatically adjust the accelerator as needed to maintain a consistent speed. We'll call this cruise control. That has a nice ring to it. Here's our initial pass at a control system. Our controller takes some input or a set point. In our example, this is like setting a particular value on our cruise control, say 100 kilometers per hour. The controller then figures out what value to send to the plant. Remember that the plant in our example is the engine. So let's create a simple mapping from desired speed to accelerator position, as we know that the car will reach a maximum speed based on that position. The pedal position will in turn affect the engine, which will accelerate the car up to some speed. Seems simple enough. This is known as an open-loop system. There is no sensor or feedback loop. While this might work in some cases, I'm sure you can already see some issues with this control scheme for our cruise control device. If we're going up a hill, the car is going to slow down thanks to the effects of gravity, even though the accelerator is in the same position. Similarly, if we're going down a hill, the car is going to speed up. So we need to let off the gas pedal in order to maintain that same speed. This is something that our open-loop system cannot accommodate. So we turn to a closed-loop system. Here, the output is measured by some sort of sensor, and that feedback is sent to our controller. In most cases, we will compare the input setpoint to that feedback to generate an error signal, which goes to the controller. We'll label that error signal as E of T, as it's something that can vary over time. In our manual example, our brains acted as the controller. We could adjust the gas pedal with our foot as needed to maintain our speed, regardless of road variations. But we want to automate this process to take that pressure off the driver. So we need to design a controller that accepts the error signal and outputs some other signal to drive the process. First, let's try simply multiplying our error value by some constant to get the output signal. This is known as a proportional controller, as the output signal is proportional to the error term. We'll start with a very simple example before coming back to our cruise control system. Instead of speed, let's say that we want to move our car 10 kilometers. So our setpoint is 10 kilometers. Our error is the difference between this setpoint and the actual position of the car. Therefore, our error is a measurement of the distance to the goal. We want the output to be the speed of our car in kilometers per hour. The plant is still our car or engine. We give it a speed and it performs the movement for us. Here's a diagram of our car moving toward its goal. At zero kilometers, the error is 10 minus zero, so 10 kilometers. Let's set our constant, kp, to some arbitrary value like 5. So we multiply 5 by that error to get our controller output, which is 50 kilometers per hour. Our car starts moving very quickly at first. Don't worry about acceleration and deceleration for this very simple example. At 3 kilometers, the error is 7 kilometers. So our speed slows to 5 times 7, or 35 kilometers per hour. We continue with this pattern. At 5 kilometers, the speed is 25 kilometers per hour, as the error is 5 kilometers. At 7 kilometers, the error is 3, so the speed slows even more to 15 kilometers per hour. Similarly, at 8 kilometers, the speed drops to 10 kilometers per hour. And at 9 kilometers, the speed slows all the way down to 5 kilometers per hour. Finally, at our goal, the speed should be 0 kilometers per hour, as our error is 0 kilometers. If we graph our error and speed, we should see something like this. The error starts at 10 kilometers, and the speed is proportionally high, at 50 kilometers per hour. As we get closer to our goal, the speed also starts to drop. As a result, we approach our goal more slowly, until we're barely moving at all. Assuming an ideal system, we should reach our goal at a snail's pace without overshooting. Note that we can adjust our constant value. For example, if we set it to 2, the speed will be slower, but still proportional to the distance error. It will take longer to reach our goal. If we increased the constant to, say, 10, we'll reach our goal much faster. But we start risking overshoot in non-ideal systems, and possibly breaking some speeding laws. Now, let's turn our attention back to the cruise control system that we're designing, as it's a little more complex than this distance problem. Remember, we want our car speed to match our set point at 100 kilometers per hour, and we need to design a controller that can take in this information and adjust the accelerator position. We'll say our accelerator position can go from 0 to 10 centimeters. 0 is when we're not touching the pedal, and 10 centimeters is pushing it in as far as it will go. For the sake of argument, we'll say that we can cruise at 100 kilometers per hour on a flat road when the accelerator is pressed in at 5 centimeters. Now, let's choose an initial proportional constant, say, 0.1. At 0 kilometers per hour, the error is 100 kilometers per hour, and the output of our controller is 0.1 times 100 to get 10 centimeters. That's pressing the accelerator all the way in, so we will accelerate quite quickly at first. Then, at 30 kilometers per hour, our error is 70. Times our constant of 0.1, that means we begin to let off the gas pedal by a few centimeters. At 50 kilometers per hour, we've moved the gas pedal to 5 centimeters. Remember, that is our assumed cruising pedal position. Do you see the problem? We're not at our desired speed. At 50 kilometers per hour and a pedal position of 5 centimeters, we'll continue to accelerate some. However, once we reach 60 kilometers per hour, our pedal position will be at 4 centimeters. For the sake of this example, we'll say that we can cruise at 60 kilometers per hour with the pedal position at 4 centimeters. But we'll never quite get to our desired speed. If our error is 0, that means our proportional controller will set the accelerator position to 0, which essentially means no power from our engine. Let's graph our error and speed. As you can see, we go from 0 up to 60 kilometers per hour, but we never quite reach our set point. The difference between our set point and actual achieved cruising speed is known as the steady state error. Many systems, like a cruise control, suffer from this steady state error. No matter how we adjust KP, we'll never quite get to the set point. We could include a constant offset, but there's a better, more robust way to handle such errors. Let's go back to our system diagram for a moment. A common way to find steady state error is to look at how much error has accumulated over time. We then multiply that amount by some new constant, KI, before adding it to the output of our proportional controller. In the math world, we would take the area under our error curve, which would be an integral. This is how we get to a PI controller. Proportional and integral values are summed together to produce a single output. However, integrating this unknown error function is often impossible for most real-world systems. So we can take an approximation of that integral. We simply sum each error term over time and then multiply that accumulated value by our KI constant. Note that for a true summation approximation, you should multiply by a delta T interval time. To make our calculations easier, we can assume that this is 1. For example, maybe we take an error reading every 1 second. Or if we know that this interval time is constant, we can just say it's part of our KI value. Either way, we won't need to worry about this term in this example. Now, let's see what happens if we include this integral term with our proportional term from the previous example. We start at 0 kilometers per hour, so our error is 100. From before, we know that our proportional controller wants to set the accelerator to 10 centimeters, assuming we have KP set to 0.1. We'll set the KI term to 0.01. As this is the start of the accumulation, the approximate integral is just 100. 100 times our KI of 0.01 is 1 centimeter. We then add the two controller outputs together to get a sum of 11 centimeters. Our accelerator doesn't go to 11, so for now, we'll just say we max out the accelerator at 10 centimeters. We start accelerating up to 30 kilometers per hour, so our error term is now 70. Our proportional controller sets the pedal to 7 centimeters. In the integral controller, we accumulate 70 on top of 100 to get 170, and multiply that by our constant to get an output of 1.7 centimeters. Our new PI controller output is 8.7 centimeters. So, we keep the accelerator pressed farther than we did with just the P controller. We continue this trend to get a PI controller output of 7.2 centimeters when going 50 kilometers per hour. Notice that the P controller is starting to contribute less, but the I controller is contributing more as the area under the error curve grows. At 70 kilometers per hour, the output is 5.5 centimeters. We've already passed our original limit of 60 kilometers per hour when we used just the P controller. Even at 70 kilometers per hour, our controller is saying to continue accelerating. However, when we reach 80 kilometers per hour, the total output has dipped below 5 centimeters, which we know is a problem. There's still an error. But wait, let's see what happens next. Next, assuming we continue at 80 kilometers per hour, the P controller will output the same amount. However, the integral controller continues to accumulate. It says to increase the value of the output to account for that error. So, we press the accelerator a little more to start closing that error. It might look like the output will fluctuate as it approaches that steady state value, but this example is just intended to show how the integral controller will account for that error. In reality, our example will look something like this. The proportional and integral terms will reach a steady state when the error is zero, which means we perfectly hit our mark. In fact, in many cases, a P.I. controller is all we need. It's enough to move a process toward its goal while accounting for steady state error. However, you'll often run into other issues that can't be solved by proportional and integral terms alone. A well-tuned P.I. controller looks something like this. However, the rate of change might be too slow for you. This is known as an overdamped response. You may want your process to approach the goal quickly. So, you start playing with the K.P. and K.I. terms, and you end up with something like this. While the system approaches the set point quickly, it also overshoots. The integral term will account for the negative error and eventually bring the output in line with the set point. However, the system might oscillate for a while before it does that. This is known as an underdamped response. In many cases, no matter how much you fiddle with the K.P. and K.I. terms, you can't seem to find a good balance between an overdamped and an underdamped response. What you're after is a response curve that looks like this. The process quickly approaches the set point and settles there with no overshoot. This is known as a critically damped response. To make this happen, we need to add one more component to our controller. This is the derivative term. We multiply the rate of change in the error by some constant, KD. We sum the proportional, integral, and derivative terms together to get the output. This is known as a PID controller. Let's see how this works with an example. The derivative of the error is the slope at that particular point. That means if the error is decreasing quickly, meaning we are accelerating fast toward our target speed, then the derivative term will be a negative number with a large magnitude. Like with the integral term, this is often difficult or impossible to analytically solve unless you have a mathematical model of your process. So we turn to numerical estimations again to help us out. We can approximate the derivative by simply subtracting the current error value from the previous error value and dividing that by our sampling interval time. That should give us the slope of the line between the current and previous values, which is good enough for what we need in most cases. And just like we did with the integral term, we can assume the sampling interval, delta T, is 1, or we can assume it's part of our constant for now. The difference between the current and previous error values will work in this instance. The idea is that we want a term that can tell if we are approaching the goal too fast and dampen the response just enough so that we don't overshoot. So let's work on another simple example. Here we've added the proportional, integral, and derivative terms. I've adjusted the ki coefficient to be a little higher. That way our speed increases much more quickly toward our goal, at least in theory. It's not a perfect example, but it should illustrate how the p, i, and d terms work together. At first, we accelerate very quickly from a stop. Note here that the derivative term is zero as we do not have a slope yet. Once again, we make the assumption that we can only push the pedal to 10 centimeters. At 30 kilometers per hour, the integral term starts to increase quickly. However, because we are accelerating so fast, the difference between this and the last error terms dampen the current pedal position to 7.1. We repeat this procedure and we see that we've dampened the pedal position again to 7 centimeters. At 70 kilometers per hour, our derivative controller output is again minus 2 centimeters, which makes the total 5.5 centimeters. We still have the pedal pressed more than what we want to cruise at, but it means we're accelerating. As we start to approach our goal, you can see how the proportional term approaches zero and the integral term takes over. The derivative term is no longer needed to dampen, so that also approaches zero. In the ideal case, we land right at 100 kilometers per hour with the pedal at 5 centimeters. We probably overshot a little as the pedal was pulled back to 4 centimeters for that second-to-last sample. That's probably just indicative of my poor example here. However, it shows that we need to tune the kp, ki, and kd terms to make a system that is critically damped where we approach our goal quickly without overshooting. If you have a mathematical model that correctly describes the behavior of the plant, you can solve for kp, ki, and kd to make the system critically damped. This model is usually expressed as a transfer function, which gives the plant's output for any given input. This works well in academic settings with simple plants. In most cases, it's either impossible or very difficult to know the exact transfer function of the plant. So, we turn to other tuning methods. There are a number of ways to tune a PID controller, but we'll demonstrate a simple manual method here. We measure the output as we vary each of the constants. You set ki and kd to zero and leave kp at some low value. You increase kp until the output oscillates with some steady state offset. You then increase ki until you correct the steady state offset, even if that means more oscillation. Just make sure the output does not become unstable and oscillates out of control. Finally, adjust kd to remove the oscillations. At the end of the process, you should have an optimally tuned system. Note there are other tuning schemes, such as the Ziegler-Nichols method, where you can use some math to help you choose ki and kd to reduce the amount of trial and error that's needed. Early PID controllers were pneumatic, and then eventually electrical. Here is an example of a classical electrical PID controller used in a research paper. However, most modern PID controllers rely on code and microcontrollers rather than discrete components. This is some pseudocode for how you might implement a PID controller. We set the constants to some arbitrary values to be tuned later, and we also need to know the interval time. This corresponds to the sample rate, how often we will sample from our sensor, and perform some action on our plant. We'll go with one millisecond. We loop forever in this case. At the beginning of the loop, we read our feedback value, which is often from a sensor. We then calculate each of the PID terms. To do that, we take the difference between the set point and the measured value to get the error term. We then multiply it by the interval to accumulate in the integral term. The derivative term takes the difference between the current and previous errors, and then divides that value by the interval. We then multiply each of these values by their respective k constants and add them together. While not shown here, you would use that output value to drive whatever you needed. This could be, say, adjusting the accelerator by some amount in centimeters. Finally, we save the error value for the next iteration and then wait for the given interval time. This is a very simple example. You could obviously do this in a separate thread with timers or using a variable interval time. You would also need to spend some time tuning the 3k constants, but this should be enough to get you started making your own PID controllers in code. Now, where would you actually use such a controller? PID controllers are used in countless applications. And as you probably guessed, most cruise control systems in cars use PID controllers. In the 1920s, Minorski was researching methods for automatic ship steering when he devised his theoretical analysis of PID controllers. They are also heavily used in manufacturing and industrial equipment. For example, many reflow ovens use PID controllers to adjust the heating elements as needed to match the set points in the profile. This allows mass soldering of components without burning boards. I hope this helps you understand the theory behind PID controllers and how they can be used to regulate a system based on feedback from one or more sensors. In a future episode, I'll demonstrate tuning a PID controller. Stay tuned and happy hacking! Stay tuned! souvenirsetten北店店